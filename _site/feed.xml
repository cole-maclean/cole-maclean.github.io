<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Online Profile</title>
    <atom:link href="http://cole-maclean.github.io//feed.xml" rel="self" type="application/rss+xml"/>
    <link>http://cole-maclean.github.io//</link>
    <description>Data visualizations capturing what I&#39;m working on, what I&#39;m learning and what I&#39;m reading.</description>
    <pubDate>Wed, 31 Aug 2016 15:38:06 +0200</pubDate>
    
      <item>
        <title>Meetup.com City Finder</title>
        <link>http://cole-maclean.github.io//blog/Meetup.com%20City%20Finder/</link>
        <guid isPermaLink="true">http://cole-maclean.github.io//blog/Meetup.com%20City%20Finder/</guid>
        <description>&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Back in February of this year, I packed up my comfortable Canadian life in Calgary, Alberta and moved to Barcelona, Spain to obtain an education in the Mediterranean lifestyle, European cultures, un poco español and some Artificial Intelligence. Having never moved to a new country on my own before, I was overwhelmed by the diverse languages and cultures of Barcelona, and for the first time, I felt truely lonely. With so many cool things to explore and experience in my new city, it took longer than I anticipated to find a group of people to share those experiences with. I realized that making new friends in your mid-twenties isn’t quite the same as it was back on the playground, so I began to break out of my comfort zone by using &lt;a href=&quot;https://www.meetup.com/&quot;&gt;meetup.com&lt;/a&gt; to attend events I found interesting. It’s been a fantastic application for finding new friends and helping me settle into my new life in Barcelona.&lt;/p&gt;

&lt;p&gt;Meetup.com provides a platform for anybody to build a community around a topic of interest, scheduling events from casual drinks in a bar, hikes in the mountains or performing in a singing group. With meetup.com, you can “Find your people” by sharing your interests with others in your local community. This platform provides an interesting opportunity to explore which world cities have the most active users for any given topic, allowing us to find our ideal cities and perhaps learn a little about different cultures around the world. Fortunately, meetup.com provides a robust &lt;a href=&quot;http://www.meetup.com/meetup_api/&quot;&gt;API&lt;/a&gt; to allow us to explore this dataset for ourselves.&lt;/p&gt;

&lt;h2&gt;The Final Visualization&lt;/h2&gt;

&lt;p&gt;The aim of this project is to build an interface to allow the user to explore various topics of interest and visualize the best cities for that unique grouping of topics. I’ve used my hometown of Calgary in the below example image with data science and spanish language as topics. The final visualization can be viewed &lt;a href=&quot;https://cole-maclean.github.io/meetupcityfinder/&quot;&gt;here.&lt;/a&gt; Feel free to go play with it, the rest of this post outlines how to retrieve the data from meetup.com using it’s API.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/calgary_example.png&quot; alt=&quot;Calgary Example&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;The Build&lt;/h2&gt;

&lt;h3&gt;Meetup.com API&lt;/h3&gt;

&lt;p&gt;Meetup.com has a very well documented API, and memebers of the community have built API wrappers for many popular programming languages. Python being my language of choice, I’ve utilized the Python &lt;a href=&quot;https://pypi.python.org/pypi/meetup-api/&quot;&gt;meetup-api&lt;/a&gt; library for this project. To use the package, we first need to obtain an &lt;a href=&quot;https://secure.meetup.com/meetup_api/key/&quot;&gt;API key&lt;/a&gt; to be used as authentication for our client object. &lt;/p&gt;

&lt;p&gt;```python
import meetup.api
import configparser
import time&lt;/p&gt;

&lt;p&gt;config = configparser.ConfigParser()
config.read(‘secrets.ini’)&lt;/p&gt;

&lt;p&gt;meetup_api_key = config.get(‘meetup’, ‘api_key’)
client = meetup.api.Client(meetup_api_key)
```&lt;/p&gt;

&lt;p&gt;What we need from the API is to iterate over all groups that exist on the platform, collect the topic tags that group organizers assign their group, and collect the member count for each group. The meetup.api python package provides a GET function to obtain a list of groups having certain input filter parameters. The request returns a results object with a list of groups and their respecitive data including topic keywords.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
groups = client.GetGroups(lat=51.0486, lon=-114.0708,radius=50,fields=[&#39;topics&#39;]) #example meetup API call with Calgary as center
                                                                                  #GPS coords and 50 mile radius. Optional data
                                                                                  #request to return each groups list of topics
groups.results[1]
&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;29/30 (10 seconds remaining)



{&#39;category&#39;: {&#39;id&#39;: 2,
  &#39;name&#39;: &#39;career/business&#39;,
  &#39;shortname&#39;: &#39;career-business&#39;},
 &#39;city&#39;: &#39;Calgary&#39;,
 &#39;country&#39;: &#39;CA&#39;,
 &#39;created&#39;: 1038423812000,
 &#39;description&#39;: &#39;&amp;lt;p&amp;gt;&amp;lt;img src=&quot;http://photos1.meetupstatic.com/photos/event/b/0/5/c/600_435105148.jpeg&quot;&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;&amp;lt;i&amp;gt;&amp;lt;b&amp;gt;PLEASE READ BEFORE JOINING OUR GROUP&amp;lt;/b&amp;gt; &amp;lt;br&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;The Calgary Business Professionals Group (&quot;CBP&quot;) is the premier business-related Meetup in the Calgary area. &amp;amp;nbsp;We cater to Calgary entrepreneurs.&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;The CBP is all about forging strong relationships with no strings attached.&amp;amp;nbsp;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;We are not trying to sell you anything!&amp;amp;nbsp;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;Our strength is our membership (over 2000 members) and our events (we have hosted over 300).&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;The CBP Meetup Group is one of the largest and longest running Meetup groups in Calgary (in existence since 2002).&amp;amp;nbsp;&amp;lt;/span&amp;gt;&amp;amp;nbsp;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Our events are invitation-only, private functions and are open to CBP Meetup Group members and their guests only.&amp;lt;/b&amp;gt; &amp;lt;br&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;Our events are free to attend (our breakfast event does require you to spend $10 minimum to support our venue).&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;There are great food and drink specials at our &quot;Beer and Wings&quot; event, but you are on the hook for what you consume (our host venue runs individual tabs for all attendees). Please tip your server!&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;&amp;lt;b&amp;gt;In the interests of &quot;good networking&quot;, we require ALL new members to include:&amp;lt;/b&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;&amp;lt;b&amp;gt;&amp;amp;nbsp;(1) A recognizable photo (headshot or similar), NO ADVERTISING OR LOGOS!&amp;lt;/b&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;&amp;lt;b&amp;gt;(2) BOTH your first AND last name on both your Meetup AND Group profiles&amp;lt;/b&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;&amp;lt;b&amp;gt;&amp;lt;i&amp;gt;Your application for membership will be rejected without this information&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;&amp;lt;b&amp;gt;In addition:&amp;lt;/b&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;(3) We do not permit members of Multi-Level Marketing (&quot;MLM&quot;) or other &quot;Network Marketing&quot; organizations to join this group.&amp;amp;nbsp;&amp;lt;/strong&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;(4) We allow members of other networking groups to join, but PLEASE, do not recruit our members or promote your events at our functions. Doing so will result in removal from CBP.&amp;lt;/strong&amp;gt; &amp;lt;br&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;(5) Please use Meetup\&#39;s messaging function to communicate to follow members but selling or promoting your product or service in an unsolicited fashion is not permitted. Any violations of this policy will result in removal from the group.&amp;amp;nbsp; &amp;lt;br&amp;gt;&amp;lt;/strong&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;We look forward to meeting you at one of our events.&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;span&amp;gt;Yours in business and entrepreneurship;&amp;lt;/span&amp;gt; &amp;lt;br&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Lisa Marie Genovese&amp;lt;/b&amp;gt;, Organizer&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Sean Phillips&amp;lt;/b&amp;gt;, Co-Organizer and Host of the monthly breakfast event&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Brad Celmainis&amp;lt;/b&amp;gt;, Co-Organizer and Host of the monthly Beer and Wings event&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt;&amp;lt;img src=&quot;http://photos4.meetupstatic.com/photos/event/b/0/6/4/600_435105156.jpeg&quot;&amp;gt;&amp;lt;/p&amp;gt;\n&amp;lt;p&amp;gt; &amp;lt;br&amp;gt;&amp;lt;/p&amp;gt;&#39;,
 &#39;group_photo&#39;: {&#39;highres_link&#39;: &#39;http://photos3.meetupstatic.com/photos/event/a/4/7/6/highres_228822102.jpeg&#39;,
  &#39;photo_id&#39;: 228822102,
  &#39;photo_link&#39;: &#39;http://photos3.meetupstatic.com/photos/event/a/4/7/6/600_228822102.jpeg&#39;,
  &#39;thumb_link&#39;: &#39;http://photos3.meetupstatic.com/photos/event/a/4/7/6/thumb_228822102.jpeg&#39;},
 &#39;id&#39;: 54766,
 &#39;join_mode&#39;: &#39;approval&#39;,
 &#39;lat&#39;: 51.04999923706055,
 &#39;link&#39;: &#39;http://www.meetup.com/businessincalgary/&#39;,
 &#39;lon&#39;: -114.08000183105469,
 &#39;members&#39;: 2071,
 &#39;name&#39;: &#39;Calgary Business Professionals Group&#39;,
 &#39;organizer&#39;: {&#39;member_id&#39;: 13068793,
  &#39;name&#39;: &#39;Lisa Marie Genovese&#39;,
  &#39;photo&#39;: {&#39;highres_link&#39;: &#39;http://photos3.meetupstatic.com/photos/member/a/3/7/highres_246782615.jpeg&#39;,
   &#39;photo_id&#39;: 246782615,
   &#39;photo_link&#39;: &#39;http://photos3.meetupstatic.com/photos/member/a/3/7/member_246782615.jpeg&#39;,
   &#39;thumb_link&#39;: &#39;http://photos3.meetupstatic.com/photos/member/a/3/7/thumb_246782615.jpeg&#39;}},
 &#39;rating&#39;: 4.47,
 &#39;state&#39;: &#39;AB&#39;,
 &#39;timezone&#39;: &#39;Canada/Mountain&#39;,
 &#39;topics&#39;: [{&#39;id&#39;: 389, &#39;name&#39;: &#39;Small Business&#39;, &#39;urlkey&#39;: &#39;smallbiz&#39;},
  {&#39;id&#39;: 1238, &#39;name&#39;: &#39;Marketing&#39;, &#39;urlkey&#39;: &#39;marketing&#39;},
  {&#39;id&#39;: 3880, &#39;name&#39;: &#39;Professional Development&#39;, &#39;urlkey&#39;: &#39;prodev&#39;},
  {&#39;id&#39;: 4422, &#39;name&#39;: &#39;Social Networking&#39;, &#39;urlkey&#39;: &#39;socialnetwork&#39;},
  {&#39;id&#39;: 15405,
   &#39;name&#39;: &#39;Business Referral Networking&#39;,
   &#39;urlkey&#39;: &#39;business-referral-networking&#39;},
  {&#39;id&#39;: 15720,
   &#39;name&#39;: &#39;Professional Networking&#39;,
   &#39;urlkey&#39;: &#39;professional-networking&#39;},
  {&#39;id&#39;: 17325,
   &#39;name&#39;: &#39;Small Business Marketing Strategy&#39;,
   &#39;urlkey&#39;: &#39;small-business-marketing-strategy&#39;},
  {&#39;id&#39;: 17635, &#39;name&#39;: &#39;Business Strategy&#39;, &#39;urlkey&#39;: &#39;business-strategy&#39;},
  {&#39;id&#39;: 19882, &#39;name&#39;: &#39;Entrepreneurship&#39;, &#39;urlkey&#39;: &#39;entrepreneurship&#39;},
  {&#39;id&#39;: 20060,
   &#39;name&#39;: &#39;Entrepreneur Networking&#39;,
   &#39;urlkey&#39;: &#39;business-entrepreneur-networking&#39;},
  {&#39;id&#39;: 20743,
   &#39;name&#39;: &#39;Small Business Owners&#39;,
   &#39;urlkey&#39;: &#39;small-business-owners&#39;},
  {&#39;id&#39;: 45636, &#39;name&#39;: &#39;Calgary Business&#39;, &#39;urlkey&#39;: &#39;calgary-business&#39;},
  {&#39;id&#39;: 65792, &#39;name&#39;: &#39;B2B Networking&#39;, &#39;urlkey&#39;: &#39;b2b-networking&#39;},
  {&#39;id&#39;: 72702,
   &#39;name&#39;: &#39;Calgary Entrepreneurs&#39;,
   &#39;urlkey&#39;: &#39;calgary-entrepreneurs&#39;}],
 &#39;urlname&#39;: &#39;businessincalgary&#39;,
 &#39;utc_offset&#39;: -21600000,
 &#39;visibility&#39;: &#39;public_limited&#39;,
 &#39;who&#39;: &#39;Business Professionals&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The meetup.com API does not provide a clear way to access every group on the platform, as the design of the system is such that a user selects a center location to search from in a geographic radius to ensure users are recommended groups close to their desired location. One option to obtain all availible groups is to input an impossibly high radius to encompass the entire globe.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
groups = client.GetGroups(lat=51.0486, lon=-114.0708,radius=10000000000,fields=[&#39;topics&#39;])
groups.meta[&#39;total_count&#39;]
&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;28/30 (7 seconds remaining)

257973
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Although this option is technically feasible, and as of today returns a total of 257,973 unique groups, the total time to make that single request is over a minute. The meetup.com API is limited to only return 200 results at a time, which  would require over 1290 of these requests. To overcome the limit of 200 results, the API provides a page offset parameter to programmatically scroll through each page containing the next 200 groups.&lt;/p&gt;

&lt;p&gt;```python
groups_per_page = 200&lt;/p&gt;

&lt;p&gt;groups = client.GetGroups(lat=51.0486, lon=-114.0708,radius=5)#initial request to obtain total request pages
time.sleep(1)
pages = int(groups.meta[‘total_count’]/groups_per_page)
print(“total request groups = “ + str(groups.meta[‘total_count’]))
for i in range(0,pages + 1): #iterate over each request page
    print(“page “ + str(i) + “/” + str(pages))
    #get offseted request by current request page
    groups = client.GetGroups(lat=51.0486, lon=-114.0708,radius=5,fields=[‘topics’],pages=groups_per_page,offset=i)
```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;29/30 (10 seconds remaining)
total request groups = 903
page 0/4
28/30 (8 seconds remaining)
page 1/4
27/30 (6 seconds remaining)
page 2/4
26/30 (5 seconds remaining)
page 3/4
25/30 (3 seconds remaining)
page 4/4
24/30 (2 seconds remaining)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Initally I attempted to use the above method to iterate over the entire 1290 pages of groups on meetup.com, but each request required me to pass the impossibly high radius parameter with each respective offset page, which I worried was hammering meetup.com’s servers and would have taken a few days of straight API requests. The alternate solution I ended up using was to obtain a list of the top 250 global cities by population and use each as the centroid GPS location with a 300 mile radius to obtain the nearby groups. The complete API requests and data structure handling done for collecting the meetup.com data can be viewed in main.py in the projects &lt;a href=&quot;https://github.com/cole-maclean/meetupcityfinder/tree/gh-pages&quot;&gt;repo.&lt;/a&gt; To keep the data within githubs maximum 100mb file limit and ensure a performant d3.js visualization, city-topics with less then 100 members are filtered out of the dataset.&lt;/p&gt;

&lt;h2&gt;The Viz&lt;/h2&gt;

&lt;p&gt;The visualization is made up of 3 main components:&lt;br /&gt;
1. Globe with city markings indicating cities containing the user selected topics and color coded by that cities sum total ranking in each topic - the lowest total score is the highest ranking city for that unique list of topics. The base for the d3.js globe was modified from &lt;a href=&quot;https://gist.github.com/serdaradali/11346541&quot;&gt;here&lt;/a&gt;&lt;br /&gt;
2. Top 10 ranking cities for user selected list of topics &lt;br /&gt;
3. Detailed data for user selected city indicating that cities top 5 ranked topics, plus the rank for each of the user inputted topics  &lt;/p&gt;

&lt;h3&gt;My Example&lt;/h3&gt;

&lt;p&gt;To test out the final viz, I added a few of the topic that interest me to find my ideal city. The following image depicts the results for my list of topics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/my_example.png&quot; alt=&quot;My Example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The best city for all 8 topics appears to be New York, which is usually the number one city for any list of topics. Meetup.com was started in New York, and I believe New York is a large percentage of the total meetup.com user base, so most queries will show it as being the top city for most given list of topics. The results past New York become more interesting for my example, especially Sydney. It does well in all of the topics that interest me, plus it ranks first in the category “fun fun fun”. Maybe I’ll have to check out Sydney once I’m done my studies here in Barcelona.&lt;/p&gt;

&lt;h2&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;The meetup.com API is a really cool dataset that can be used to explore the world and how people are interacting in it. There’s probably alot more that can be done with this dataset, and I’m looking forward to playing with it some more to find further insights and interesting anomolies. Hopefully others have fun finding their ideal city and exploring the world through this lens.&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Aug 2016 00:00:00 +0200</pubDate>
      </item>
    
      <item>
        <title>Tesla Supercharger Network Predictive Modeling</title>
        <link>http://cole-maclean.github.io//blog/Tesla%20Supercharger%20Network%20Predictive%20Model/</link>
        <guid isPermaLink="true">http://cole-maclean.github.io//blog/Tesla%20Supercharger%20Network%20Predictive%20Model/</guid>
        <description>&lt;p&gt;&lt;img src=&quot;/img/SCnetwork.gif&quot; alt=&quot;Network&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;With the recent unveiling of Tesla’s Model 3 and pre-orders approaching 400,000, the internet has been buzzing with Tesla discussions and analysis. One of Tesla’s key differentiators from other mass market Electric Vehicals (EVs) is its Super Charger (SC) network that provides 170 miles of range in 30 minutes &lt;a href=&quot;https://www.teslamotors.com/supercharger&quot;&gt;source.&lt;/a&gt; With Elon Musk stating plans to double the size of the SC network by the end of 2017, a large amount of planning, resources and investment are being allocated to this network expansion.&lt;/p&gt;

&lt;p&gt;An analysis to build a predictive mode for Tesla’s Supercharger network expansion was performed using &lt;a href=&quot;http://beakernotebook.com/&quot;&gt;beaker notebooks.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The full analysis can be viewed &lt;a href=&quot;https://lab.beakernotebook.com/publications/c93f91bc-1cc7-11e6-b331-2b7c0d919ca8&quot;&gt;here.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The interactive network data visualization can be viewed &lt;a href=&quot;http://cole-maclean.github.io/MAI-CN/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 29 Jun 2016 00:00:00 +0200</pubDate>
      </item>
    
      <item>
        <title>Tesla Supercharger Network Exploratory Data Analysis</title>
        <link>http://cole-maclean.github.io//blog/Tesla%20Supercharger%20Network%20Exploratory%20Data%20Analysis/</link>
        <guid isPermaLink="true">http://cole-maclean.github.io//blog/Tesla%20Supercharger%20Network%20Exploratory%20Data%20Analysis/</guid>
        <description>&lt;p&gt;&lt;img src=&quot;/img/network_thumb.PNG&quot; alt=&quot;Network&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;With the recent unveiling of Tesla’s Model 3 and pre-orders approaching 400,000, the internet has been buzzing with Tesla discussions and analysis. One of Tesla’s key differentiators from other mass market Electric Vehicals (EVs) is its Super Charger (SC) network that provides 170 miles of range in 30 minutes &lt;a href=&quot;https://www.teslamotors.com/supercharger&quot;&gt;source.&lt;/a&gt; With Elon Musk stating plans to double the size of the SC network by the end of 2017, a large amount of planning, resources and investment are being allocated to this network expansion.&lt;/p&gt;

&lt;p&gt;An analysis exploring Tesla’s Supercharger network was performed using &lt;a href=&quot;http://beakernotebook.com/&quot;&gt;beaker notebooks&lt;/a&gt; and is currently the top rated notebook published on the platform.&lt;/p&gt;

&lt;p&gt;The full analysis can be viewed &lt;a href=&quot;https://lab.beakernotebook.com/publications/6451bcd8-096e-11e6-8b45-2b70e59230e5&quot;&gt;here.&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 21 Jun 2016 00:00:00 +0200</pubDate>
      </item>
    
      <item>
        <title>Classifying Distracted Driver Images</title>
        <link>http://cole-maclean.github.io//blog/Classifying%20Distracted%20Driver%20Images/</link>
        <guid isPermaLink="true">http://cole-maclean.github.io//blog/Classifying%20Distracted%20Driver%20Images/</guid>
        <description>&lt;p&gt;The final report PDF for the techniques used in this competetion can be downloaded &lt;a href=&quot;/blog/files/kaggle-distracted-driver.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 09 Jun 2016 00:00:00 +0200</pubDate>
      </item>
    
      <item>
        <title>Growth of the Scientific Boundary</title>
        <link>http://cole-maclean.github.io//blog/Growth%20of%20the%20Scientific%20Boundary/</link>
        <guid isPermaLink="true">http://cole-maclean.github.io//blog/Growth%20of%20the%20Scientific%20Boundary/</guid>
        <description>&lt;p&gt;Visualization can be viewed &lt;a href=&quot;http://cole-maclean.github.io/Udacity-DSNDP6/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;This visualization attempts to encode the growth in size and scope of the scientific boundary for multiple disciplines by using the count of published scientific papers to the Arxiv pre-publishing website as a proxy for the “size” of a given scientific boundary. Meta data is collected using &lt;a href=&quot;http://arxiv.org/help/api/index&quot;&gt;Arxiv’s API&lt;/a&gt;. The titles of each paper in a given discipline and for a given year are passed into a “Bag of Words” categorization model that sorts each paper based on its abstract into a category, or defines a new category if a new cluster in the model emerges as more paper titles are added as the years progress. Each new category that is created is “born” from a parent category, which is determined as being the existing category that is most-like the newly created category, based on the words in the labels of each category. The bubbles representing each category grow in size based on the fraction of papers within that category, and are colored according to its parent category. The goal is to not only visually represent the growth in scale of knowledge within a scientific discipline, but also the fragmentation of a discipline into greater numbers of unique specializations.&lt;/p&gt;

&lt;h2&gt;Design&lt;/h2&gt;

&lt;h3&gt;Paper Count Encoding&lt;/h3&gt;

&lt;p&gt;The original design was to utilize a Mandelbrot fractal to encode the growing boundary.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/madelbrot_fractal.png&quot; alt=&quot;Mandelbrot Fractal&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After obtaining feedback from co-workers on the original sketch, many suggested that the complexity of the fractal image took away from the main intent of the visualization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/fractal_sketch.JPG&quot; alt=&quot;Fractal Sketch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using this feedback, the design evolved into a simpler bubble representation of the dataset. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/bubble_sketch.JPG&quot; alt=&quot;Bubble Sketch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This simpler representation of the dataset provides intuitive understanding of the data remaining visually appealing.&lt;/p&gt;

&lt;h3&gt;Legend&lt;/h3&gt;

&lt;p&gt;Originally, each category was uniquely colored and identified in a legend. After reviewing, it was discovered there were too many categories to effectively distinguish using color and also that coloring each category did not provide additional information as a user could identify each category with a tooltip label. The decision was made to color each category based on its parent category. This providing additional context to the visualization, providing the ability to quickly identify which categories developed from a given parent category.&lt;/p&gt;

&lt;h2&gt;Feedback&lt;/h2&gt;

&lt;h3&gt;Initial Feedback&lt;/h3&gt;

&lt;p&gt;Before formally developing the visualization, I received feedback on the conceptual sketch of the Mandelbrot fractal representation. Showing this to a couple co-workers, the main feedback received was the apparent complexity of the visualization and whether a simpler representation would be just as effective. Using this feedback, I did some research of d3.js visualizations looking for inspiration and stumbled upon &lt;a href=&quot;http://bl.ocks.org/mbostock/4063269&quot;&gt;this&lt;/a&gt; visualization that seemed suitable for my purposes. Incorporating this feedback into the design led to the current final version of the visualization&lt;/p&gt;

&lt;h3&gt;Original Published Visualization Feedback&lt;/h3&gt;

&lt;p&gt;After building out the conceptual visualization using d3.js and publishing, I received feedback from akmoore on the Udacity forums that included:
1. Adding visualization description detail to the main visualization page
2. Animating the year slider
3. Adding a stemmer to the categorization model
4. Keeping the parent category legend color consistent as the years progress&lt;/p&gt;

&lt;p&gt;I incorporated all but 1 of the recommendations in the final design. Although the suggestion 4. to keep the parent category legend colors consistent is a valid recommendation, it requires a trade-off in the d3.js implementation of the visualization to either keep the colors consistent, or keep the categories with the same parent category spatially close to each other. After reviewing the suggestion, I decided to keep the categories with the same parent categories spatially close as to allow the user to easily identify categories with the same parent category.&lt;/p&gt;

&lt;h3&gt;Initial Submission Feedback&lt;/h3&gt;

&lt;p&gt;The initial submission for this project recieved 6 points of feedback:
1. Keeping the parent category legend color consistent as the years progress - this suggestion was implemented and successfully maintains color consistency while keeping categories with the same parent category spatially close to each other
2. Keep relative circle size based on paper count consistent - Recommendation implemented by changing the “category_ratio” scale from log to sqrt. Although still not perfect, it provides a balance between keeping sizes relatively similar but preventing really small categories from being so tiny that they are barely visible.
3. Add details about number displayed in circle - Updated title to provide more detail
4. Add extra hint that slider can be manipulated by user - Slider animation and Cross-Hair selector should give enough clues to user
5. Bug in header prevent slider selection - Updated title to prevent this bug
6. Usability bugs with tooltip and random paper details - Updated tooltip to descripe a random paper is selected and removed bug that makes tooltip reappear after mouseleave event.&lt;/p&gt;

&lt;h2&gt;Resources&lt;/h2&gt;

&lt;p&gt;Code modified and inspired from:
    http://bl.ocks.org/mbostock/4063269&lt;br /&gt;
    http://bl.ocks.org/zanarmstrong/ddff7cd0b1220bc68a58&lt;br /&gt;
    http://colorbrewer2.org/&lt;br /&gt;
    http://www.d3noob.org/2013/01/adding-tooltips-to-d3js-graph.html&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Nov 2015 00:00:00 +0100</pubDate>
      </item>
    
      <item>
        <title>Identifying Fraud from Emails using Machine Learning</title>
        <link>http://cole-maclean.github.io//blog/Identifying%20Fraud%20from%20Emails%20using%20Machine%20Learning/</link>
        <guid isPermaLink="true">http://cole-maclean.github.io//blog/Identifying%20Fraud%20from%20Emails%20using%20Machine%20Learning/</guid>
        <description>&lt;h1&gt;Identifying Fraud from Enron Emails&lt;/h1&gt;

&lt;h2&gt;Objective&lt;/h2&gt;

&lt;p&gt;Using the Enron email corpus data to extract and engineer model features, we will attempt to develop a classifier able to identify a “Person of Interest” (PoI) that may have been involved or had an impact on the fraud that occured within the Enron scandal. A list of known PoI has been hand generated from &lt;a href=&quot;http://usatoday30.usatoday.com/money/industries/energy/2005-12-28-enron-participants_x.htm&quot;&gt;this&lt;/a&gt; USATODAY article by the friendly folks at Udacity, who define a PoI as individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity. We will use these PoI labels with the Enron email corpus data to develop the classifier.  &lt;/p&gt;

&lt;h2&gt;Data Structure&lt;/h2&gt;

&lt;p&gt;The dataset used in this analysis was generated by Udacity and is a dictionary with each person’s name in the dataset being the key to each data dictionary. The data dictionaries have the following features:&lt;/p&gt;

&lt;p&gt;financial features: [‘salary’, ‘deferral_payments’, ‘total_payments’, ‘loan_advances’, ‘bonus’, ‘restricted_stock_deferred’, ‘deferred_income’, ‘total_stock_value’, ‘expenses’, ‘exercised_stock_options’, ‘other’, ‘long_term_incentive’, ‘restricted_stock’, ‘director_fees’] (all units are in US dollars)&lt;/p&gt;

&lt;p&gt;email features: [‘to_messages’, ‘email_address’, ‘from_poi_to_this_person’, ‘from_messages’, ‘from_this_person_to_poi’, ‘poi’, ‘shared_receipt_with_poi’] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)&lt;/p&gt;

&lt;p&gt;POI label: [‘poi’] (boolean, represented as integer)&lt;/p&gt;

&lt;h2&gt;Model Development Plan&lt;/h2&gt;

&lt;p&gt;Developing the classification model will consist of 6 steps that can be iterated over until an acceptable model performance is obtained. These steps include:
1. Exploratory Data Analysis on the dataset to understand the data and investigate informative features
2. Select features likely to provide predictive power in the classification model
3. Clean or remove erroneous and outlier data from the selected features
4. Engineer/transform selected features into new features appropriate for classification modelling
5. Test different classifiers and review performance
6. Investigate new data sources that may exist to provide better model performance&lt;/p&gt;

&lt;h1&gt;Data Exploration and Cleaning&lt;/h1&gt;

&lt;h2&gt;Data Overview&lt;/h2&gt;

&lt;p&gt;The dataset includes 146 unique individials having 21 features extracted from the email corpus. The amount of feature data for each individual varies. Table 1 summarizes the data count for each feature in the dataset and the count of PoI included in non-nan feature values.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Feature&lt;/th&gt;
      &lt;th&gt;Data Count&lt;/th&gt;
      &lt;th&gt;Count PoI&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;name&lt;/td&gt;
      &lt;td&gt;146&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;poi&lt;/td&gt;
      &lt;td&gt;146&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;total_stock_value&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;total_payments&lt;/td&gt;
      &lt;td&gt;125&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;email_address&lt;/td&gt;
      &lt;td&gt;111&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;restricted_stock&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;exercised_stock_options&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;salary&lt;/td&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;expenses&lt;/td&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;other&lt;/td&gt;
      &lt;td&gt;93&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;to_messages&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;shared_receipt_with_poi&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;from_messages&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;from_this_person_to_poi&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;from_poi_to_this_person&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;bonus&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;long_term_incentive&lt;/td&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;deferred_income&lt;/td&gt;
      &lt;td&gt;49&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;deferral_payments&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;restricted_stock_deferred&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;director_fees&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;loan_advances&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;There are some features in the dataset that having missing information that will be important to our usecase. Some of the features in the dataset will not be very useful in the classification model, as they do not have labelled PoI in their subset of availible data, such as restricted_stock_deferred and director_fees. loan_advances is such a small data sample that it will likely not provide statistically signifigant predictive strength. deferral_payments is borderline, but there are other feature datasets that will likely provide better predictive information. Reviewing the data for missing values, we will exclude restricted_stock_deferred, director_fees, loan_advances and deferral_payments as features from the model.&lt;/p&gt;

&lt;h2&gt;Outliers&lt;/h2&gt;

&lt;p&gt;In analyzing the histograms for each feature, a large outlier was noticed across many variables. After further investigation, this turned out to be attributed to an aggreate row of the dataset labelled “TOTAL”. This was removed from the dataset.&lt;/p&gt;

&lt;p&gt;It also appears that ‘LAY KENNETH L’ is a large outlier in many features. Kenneth is a PoI however, so we will keep his data for building the classifier and see if we can maintain the ability to build a generalized classifier for the other PoI, as well as for Kenneth.&lt;/p&gt;

&lt;h2&gt;Data Imputation&lt;/h2&gt;

&lt;p&gt;In reviewing the dataset, many of the email feature datasets only contain 14 of the 18 identified PoI, while most of the financial features have the full 18 or 17 PoI included. Throwing out data for 20% of our identified PoI seems unnacceptable for such a scarce dataset. Imputing the mean value for any missing email feature seems appropriate, as there is likely a reasonable average number of emails any one employee might send. We will also impute 0 for any financial feature, making the assumption that if the value is missing the employee did not recieve that form of financial compensation. A review of this data imputation may be required as the model is developed and in analyzing the results.&lt;/p&gt;

&lt;h1&gt;Feature Selection and Engineering&lt;/h1&gt;

&lt;h2&gt;Selection&lt;/h2&gt;

&lt;p&gt;As was discussed above, restricted_stock_deferred, director_fees, loan_advances and deferral_payments have been ommitted as features for the classification model due to their small sample size. There also exists features that are labels instead of useful datum, such as name and email_address. These will not be used as features in the classification model. This leaves us with 15 potentially useful features to select from. Reviewing the data structure, there are two general categories of data: Finanical and Meta-Email. These overarching data categories seem well suited for Primary Component Analysis, to build a featureset that encompasses the most predictive information from the features and avoids dependant features such as total_payments and salary causing erroneous classification or overfitting. Applying PCA to the 15 features to reduce the dimensioanilty to 2 overarching categories, we can visualize the transformed feature relationships.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
%matplotlib inline
import numpy as np
import pandas as pd
import pickle
from ggplot import *
from feature_format import featureFormat, targetFeatureSplit
with open(&quot;my_dataset.pkl&quot;, &quot;r&quot;) as data_file:
    my_dataset = pickle.load(data_file)
with open(&quot;my_feature_list.pkl&quot;, &quot;r&quot;) as data_file:
    features_list = pickle.load(data_file)
data = featureFormat(my_dataset, features_list, sort_keys = True)
labels, features = targetFeatureSplit(data)
#Apply PCA to dataset to allow for visualization
from sklearn.decomposition import PCA
reduced_data = PCA(n_components=2).fit_transform(features)
labels = np.array(labels).reshape(len(reduced_data[:,0]),1)
reduced_data = np.hstack((reduced_data,labels))
df = pd.DataFrame(reduced_data,columns = [&#39;param1&#39;, &#39;param2&#39;,&#39;poi&#39;])
pca_plt = ggplot(df, aes(x=&#39;param1&#39;, y=&#39;param2&#39;, color=&#39;poi&#39;)) +\
    geom_point()
print pca_plt
pca_plt_zoomed = pca_plt + xlim(-2500000,5000000) +ylim(-1000000,3500000)
print pca_plt_zoomed
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/output_4_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (32479948)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/output_4_2.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (32864214)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reviewing the PCA plots, there appears to be a few PoI outliers that should be easy to classify, but zooming in on the cluster of datapoints shows PoI data points are very commingled with non-PoI data points that may be difficult to adequately classify. This model may need more advanced features to develop an adequate classifier.&lt;/p&gt;

&lt;h2&gt;Feature Engineering&lt;/h2&gt;

&lt;p&gt;It is likely that additional features are required to adequately model a PoI classifier. Most of the supplied features are absolute values of a persons’ financial compensation or email correspondance. It makes sense that a more relative measure of these features is appropriate to use when comparing whether someone is a PoI or not. Three additional features have been engineered for use in classifier development:&lt;br /&gt;
1. perc_salary - The percentage each employees’ salary makes of of their total payment, defined as salary/total_payments. This will represent if the employees compensation is made up of complex additional payments or just a basic salary.
2. perc_to_poi - The percentage of employees total sent emails to a PoI. This will represent the degree an employee spends their time interacting with a PoI.
3. perc_from_poi - The percentage of employees total recieved emails from a PoI. This will represent the degree a PoI spends their time interacting with an employee. &lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
eng_data = np.hstack((data[:,(16,17,18)],labels))
df = pd.DataFrame(eng_data,columns = [&#39;perc_salary&#39;, &#39;perc_to_poi&#39;,&#39;perc_from_poi&#39;,&#39;poi&#39;])
print ggplot(df, aes(x=&#39;perc_salary&#39;, y=&#39;perc_to_poi&#39;, color=&#39;poi&#39;)) +\
    geom_point()
print ggplot(df, aes(x=&#39;perc_salary&#39;, y=&#39;perc_from_poi&#39;, color=&#39;poi&#39;)) +\
    geom_point()
print ggplot(df, aes(x=&#39;perc_to_poi&#39;, y=&#39;perc_from_poi&#39;, color=&#39;poi&#39;)) +\
geom_point()
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/output_7_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (36868016)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/output_7_2.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (35575721)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/output_7_4.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (36864902)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In reviewing the plots, both the perc_to_poi vs perc_salary and the perc_from_poi vs perc_to_poi plots show good seperation of PoI from non-PoI datapoints. The perc_from_poi vs perc_salary plot shows less of a seperation, with many PoI data points closely clustered with non-PoI datum. These advanced features seem to show more promise then the simple PCA approach above, but we will test and compare the relative performance of tuned classifiers using the two approaches. &lt;/p&gt;

&lt;h1&gt;Model Development, Tuning and Evaluation&lt;/h1&gt;

&lt;p&gt;Using sklearn’s &lt;a href=&quot;http://scikit-learn.org/stable/tutorial/machine_learning_map/&quot;&gt;Machine Learning Map&lt;/a&gt; as a guide, we will test the PCA and advanced feature datasets with various classifier algorithms and tuning parameters in an attempt to develop the optimal PoI classifier. Sklearn’s &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html&quot;&gt;pipeline&lt;/a&gt; and &lt;a href=&quot;http://scikit-learn.org/stable/modules/grid_search.html&quot;&gt;gridsearch&lt;/a&gt; modules will be helpful in performing this analysis. We will also use sklearns StandardScaler to scale out input datasets, as some of our classifier algorithms highly recommend the use of scalled variables, like the svm classifer which is not scale invariant.&lt;/p&gt;

&lt;h2&gt;Model Validation&lt;/h2&gt;

&lt;p&gt;In order to determine which model has the “best” performance, model validation is required to prove the models ability to make correct predictions on a generalized dataset. Model validation is the method of extracting a subset of a dataset, known as the test data, training a model excluding this extraction and feeding the model with the extracted dataset to test its performance in predicting the known results of the extracted dataset. This is important to allow for the quantified comparasion of different models.&lt;/p&gt;

&lt;p&gt;To compare the performance of each tuned model, we will use Udacity’s test_classifier function. This function uses Precision and Recall as the performance metrics for each model. The Precision of the classifier is its ability to correctly identify PoI without incorrectly labelling people that are not PoI as a PoI. The Recall of the classifier is its ability to correctly identify all of the PoI that should be identified. In order to perform the validation classification, the test function uses sklearns StratifiedShuffleSplit method to split the dataset into training and test data. This allows us to maximize the data points used in both the training and testing datasets by performing multiple training and validation experiments on a shuffled subset of the full dataset. Being a fairly sparse dataset with only 18 labelled examples of PoIs, it is important for the testing methodology to perform on multiple interations of training and test data to ensure accurate performance results. It is also important to include shuffling in the test data selection to ensure a random distribution of PoI and non-PoI data points in each dataset.&lt;/p&gt;

&lt;h2&gt;PCA Featureset Model&lt;/h2&gt;

&lt;p&gt;```python
from sklearn.pipeline import Pipeline
from sklearn import svm
from sklearn.grid_search import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.cross_validation import train_test_split
from sklearn.decomposition import PCA
import tester&lt;/p&gt;

&lt;p&gt;data = featureFormat(my_dataset, features_list, sort_keys = True)
labels, features = targetFeatureSplit(data)&lt;/p&gt;

&lt;p&gt;pca_svm = Pipeline([(‘pca’,PCA(n_components=2)),(‘scaler’,StandardScaler()),(‘svm’,svm.SVC())])
param_grid = ([{‘svm__C’: [1000,10000],
                ‘svm__gamma’: [0.01,0.0001],
                ‘svm__degree’:[2,3],
                ‘svm__kernel’: [‘linear’,’rbf’,’poly’]}])
svm_clf = GridSearchCV(pca_svm,param_grid,scoring=’recall’).fit(features,labels).best_estimator_
pca_knb = Pipeline([(‘pca’,PCA(n_components=2)),(‘scaler’,StandardScaler()),(‘knb’,KNeighborsClassifier())])
param_grid = ([{‘knb__n_neighbors’: [4,5,6]}])
knb_clf = GridSearchCV(pca_knb,param_grid,scoring=’recall’).fit(features,labels).best_estimator_
pca_rfst = Pipeline([(‘pca’,PCA(n_components=2)),(‘scaler’,StandardScaler()),
                 (‘rfst’,RandomForestClassifier())])
param_grid = ([{‘rfst__n_estimators’: [4,5,6]}])
rfst_clf = GridSearchCV(pca_rfst,param_grid,scoring=’recall’).fit(features,labels).best_estimator_&lt;/p&gt;

&lt;p&gt;print svm_clf
tester.test_classifier(svm_clf,my_dataset,features_list)
print knb_clf
tester.test_classifier(knb_clf,my_dataset,features_list)
print rfst_clf
tester.test_classifier(rfst_clf,my_dataset,features_list)&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;pca&#39;, PCA(copy=True, n_components=2, whiten=False)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;svm&#39;, SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0, degree=2,
  gamma=0.01, kernel=&#39;rbf&#39;, max_iter=-1, probability=False,
  random_state=None, shrinking=True, tol=0.001, verbose=False))])
	Accuracy: 0.87573	Precision: 0.68579	Recall: 0.12550	F1: 0.21217	F2: 0.15001
	Total predictions: 15000	True positives:  251	False positives:  115	False negatives: 1749	True negatives: 12885

Pipeline(steps=[(&#39;pca&#39;, PCA(copy=True, n_components=2, whiten=False)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;knb&#39;, KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
           metric_params=None, n_neighbors=4, p=2, weights=&#39;uniform&#39;))])
	Accuracy: 0.86807	Precision: 0.54393	Recall: 0.06500	F1: 0.11612	F2: 0.07889
	Total predictions: 15000	True positives:  130	False positives:  109	False negatives: 1870	True negatives: 12891

Pipeline(steps=[(&#39;pca&#39;, PCA(copy=True, n_components=2, whiten=False)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;rfst&#39;, RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=6, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False))])
	Accuracy: 0.83913	Precision: 0.25150	Recall: 0.10450	F1: 0.14765	F2: 0.11833
	Total predictions: 15000	True positives:  209	False positives:  622	False negatives: 1791	True negatives: 12378
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reviewing the results, we see that the svm and KNeighbors classifiers have fairly good Precision, meaning they are able to properly identify PoI wihout too many incorrectly labelled PoI. The RandomForestClassiefier has poorer Precision performance. The Recall for all classifiers is rather poor, with the svm classifier having the highest Recall at 0.1255. This indicates that, although the classifiers do not miss-label many non-PoI employees, they tend to under-label correctly people that should be considered PoI. Considering the use case for this model is likely to be a filter that could provide a short-list of people to investigate further, having a high Recall is important to ensure the short-list captures people that are likely to be PoI. Given this, it is likely the above classifiers are not performant enough to provide sufficient value in this investigation. From visual inspection it was seen that the data relationships showed little classifcation potential and after exhaustively tuning these classifiers, that interpretation seems to be correct. The PCA featureset does not seem to provide enough informative characteristics to build a suitable classifier, and further feature engineering is likely required.&lt;/p&gt;

&lt;h2&gt;Engineered Feature Model&lt;/h2&gt;

&lt;p&gt;```python
from sklearn.feature_selection import SelectKBest&lt;/p&gt;

&lt;p&gt;eng_svm = Pipeline([(‘scaler’,StandardScaler()),(‘kbest’,SelectKBest()),(‘svm’,svm.SVC())])
param_grid = ([{‘kbest__k’:[3,4,5,6],
                ‘svm__C’: [1,10,100,1000],
                ‘svm__gamma’: [1,0.1,0.01,0.001],
                ‘svm__degree’:[2,3,4],
                ‘svm__kernel’: [‘linear’,’rbf’,’poly’]}])
svm_clf = GridSearchCV(eng_svm,param_grid,scoring=’recall’).fit(features,labels).best_estimator_
eng_knb = Pipeline([(‘scaler’,StandardScaler()),(‘kbest’,SelectKBest()),(‘knb’,KNeighborsClassifier())])
param_grid = ([{‘kbest__k’:[3,4,5,6],’knb__n_neighbors’: [2,3,4,5,6]}])
knb_clf = GridSearchCV(eng_knb,param_grid,scoring=’recall’).fit(features,labels).best_estimator_
eng_rfst = Pipeline([(‘scaler’,StandardScaler()),(‘kbest’,SelectKBest()),
                 (‘rfst’,RandomForestClassifier())])
param_grid = ([{‘kbest__k’:[3,4,5,6],’rfst__n_estimators’: [2,3,4,5,6]}])
rfst_clf = GridSearchCV(eng_rfst,param_grid,scoring=’recall’).fit(features,labels).best_estimator_&lt;/p&gt;

&lt;p&gt;print svm_clf
tester.test_classifier(svm_clf,my_dataset,features_list)
print knb_clf
tester.test_classifier(knb_clf,my_dataset,features_list)
print rfst_clf
tester.test_classifier(rfst_clf,my_dataset,features_list)
```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;kbest&#39;, SelectKBest(k=6, score_func=&amp;lt;function f_classif at 0x000000001F36F128&amp;gt;)), (&#39;svm&#39;, SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, degree=4, gamma=1,
  kernel=&#39;poly&#39;, max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False))])
	Accuracy: 0.81753	Precision: 0.25351	Recall: 0.18950	F1: 0.21688	F2: 0.19958
	Total predictions: 15000	True positives:  379	False positives: 1116	False negatives: 1621	True negatives: 11884

Pipeline(steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;kbest&#39;, SelectKBest(k=3, score_func=&amp;lt;function f_classif at 0x000000001F36F128&amp;gt;)), (&#39;knb&#39;, KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
           metric_params=None, n_neighbors=3, p=2, weights=&#39;uniform&#39;))])
	Accuracy: 0.86127	Precision: 0.46341	Recall: 0.25650	F1: 0.33022	F2: 0.28165
	Total predictions: 15000	True positives:  513	False positives:  594	False negatives: 1487	True negatives: 12406

Pipeline(steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;kbest&#39;, SelectKBest(k=4, score_func=&amp;lt;function f_classif at 0x000000001F36F128&amp;gt;)), (&#39;rfst&#39;, RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_l...n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False))])
	Accuracy: 0.83020	Precision: 0.32066	Recall: 0.24450	F1: 0.27745	F2: 0.25669
	Total predictions: 15000	True positives:  489	False positives: 1036	False negatives: 1511	True negatives: 11964
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Although we managed to improve our best Recall score with a tuned KNeibors Classifier, the Precision performance was dramitacally reduced. At this point, it would likely be beneficial to perform anothet iteration of the model development process in order to better explore the data, develop features and investigate other classisifier algorithms. However, there is one more approach that may be useful to investigate: developing a hybrid model by combining the two input theories outlined above.&lt;/p&gt;

&lt;h2&gt;Hybrid Model&lt;/h2&gt;

&lt;p&gt;Using sklearns &lt;a href=&quot;http://scikit-learn.org/stable/modules/pipeline.html&quot;&gt;FeatureUninion&lt;/a&gt; module, we can combine the PCA reduced dimensionality dataset with the engineered feature dataset as a hybrid featureset for model development. &lt;/p&gt;

&lt;p&gt;```python
from sklearn.pipeline import FeatureUnion&lt;/p&gt;

&lt;p&gt;combined_features = FeatureUnion([(“pca”, PCA()), (“kbest”, SelectKBest())])&lt;/p&gt;

&lt;p&gt;hybrid_svm = Pipeline([(‘features’,combined_features),(‘scaler’,StandardScaler()),(‘svm’,svm.SVC())])
param_grid = ([{‘features__pca__n_components’:[2,3,4,5,6,7],’features__kbest__k’:[2,3,4,5,6,7],
                ‘svm__C’: [1,10,100,1000],
                ‘svm__gamma’: [1,0.1,0.01,0.001],
                ‘svm__degree’:[2,3,4],
                ‘svm__kernel’: [‘rbf’,’poly’]}])
svm_clf = GridSearchCV(hybrid_svm,param_grid,scoring=’recall’).fit(features,labels).best_estimator_
hybrid_knb = Pipeline([(‘features’,combined_features),(‘scaler’,StandardScaler()),(‘knb’,KNeighborsClassifier())])
param_grid = ([{‘features__pca__n_components’:[2,3,4,5,6],’features__kbest__k’:[2,3,4,5,6],’knb__n_neighbors’: [1,2,3,4,5,6,7]}])
knb_clf = GridSearchCV(hybrid_knb,param_grid,scoring=’recall’).fit(features,labels).best_estimator_
hybrid_rfst = Pipeline([(‘features’,combined_features),(‘scaler’,StandardScaler()),
                 (‘rfst’,RandomForestClassifier())])
param_grid = ([{‘features__pca__n_components’:[2,3,4,5,6],’features__kbest__k’:[2,3,4,5,6],’rfst__n_estimators’: [2,3,4,5,6,7]}])
rfst_clf = GridSearchCV(hybrid_rfst,param_grid,scoring=’recall’).fit(features,labels).best_estimator_&lt;/p&gt;

&lt;p&gt;print svm_clf
tester.test_classifier(svm_clf,my_dataset,features_list)
print knb_clf
tester.test_classifier(knb_clf,my_dataset,features_list)
print rfst_clf
tester.test_classifier(rfst_clf,my_dataset,features_list)
```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;features&#39;, FeatureUnion(n_jobs=1,
       transformer_list=[(&#39;pca&#39;, PCA(copy=True, n_components=7, whiten=False)), (&#39;kbest&#39;, SelectKBest(k=5, score_func=&amp;lt;function f_classif at 0x000000001F36F128&amp;gt;))],
       transformer_weights=None)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with...y&#39;, max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False))])
	Accuracy: 0.82167	Precision: 0.33234	Recall: 0.33450	F1: 0.33342	F2: 0.33407
	Total predictions: 15000	True positives:  669	False positives: 1344	False negatives: 1331	True negatives: 11656

Pipeline(steps=[(&#39;features&#39;, FeatureUnion(n_jobs=1,
       transformer_list=[(&#39;pca&#39;, PCA(copy=True, n_components=4, whiten=False)), (&#39;kbest&#39;, SelectKBest(k=6, score_func=&amp;lt;function f_classif at 0x000000001F36F128&amp;gt;))],
       transformer_weights=None)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;knb&#39;, KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
           metric_params=None, n_neighbors=1, p=2, weights=&#39;uniform&#39;))])
	Accuracy: 0.80880	Precision: 0.22462	Recall: 0.17700	F1: 0.19799	F2: 0.18484
	Total predictions: 15000	True positives:  354	False positives: 1222	False negatives: 1646	True negatives: 11778

Pipeline(steps=[(&#39;features&#39;, FeatureUnion(n_jobs=1,
       transformer_list=[(&#39;pca&#39;, PCA(copy=True, n_components=2, whiten=False)), (&#39;kbest&#39;, SelectKBest(k=5, score_func=&amp;lt;function f_classif at 0x000000001F36F128&amp;gt;))],
       transformer_weights=None)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with...n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False))])
	Accuracy: 0.83400	Precision: 0.32771	Recall: 0.23300	F1: 0.27236	F2: 0.24729
	Total predictions: 15000	True positives:  466	False positives:  956	False negatives: 1534	True negatives: 12044
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Hybrid model was able to develop a classifier that satisfies the minimun target of 0.3 for both Precision and Recall with an SVM classifier. Although the model’s Precision is reduced compared to previous iterations, it is better suited for correctly identifier employees that are PoI in order to develop a short-list of people to investigate further.&lt;/p&gt;

&lt;h2&gt;Final Model&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;python
import operator
combined_features = FeatureUnion([(&quot;pca&quot;, PCA(n_components=7)), (&quot;kbest&quot;, SelectKBest(k=6))])
final_svm = Pipeline([(&#39;features&#39;,combined_features),(&#39;scaler&#39;,StandardScaler()),
                    (&#39;svm&#39;,svm.SVC(C=1,degree=3,kernel=&#39;poly&#39;,gamma=1))])
svm_clf = final_svm.fit(features,labels)
feature_scores = sorted({features_list[i]:svm_clf.get_params()[&#39;features&#39;].get_params()[&#39;kbest&#39;].scores_[i]
                         for i in range(0,18)}.items(),reverse=True, key=operator.itemgetter(1))
print feature_scores
print svm_clf.get_params()
tester.test_classifier(svm_clf,my_dataset,features_list)
&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[(&#39;restricted_stock&#39;, 25.380105299760199), (&#39;from_poi_to_this_person&#39;, 24.752523020258508), (&#39;other&#39;, 21.327890413979102), (&#39;exercised_stock_options&#39;, 18.861795316466416), (&#39;perc_salary&#39;, 16.719777335704574), (&#39;long_term_incentive&#39;, 11.732698076065354), (&#39;bonus&#39;, 10.222904205832778), (&#39;total_payments&#39;, 9.4807432034789336), (&#39;total_stock_value&#39;, 8.9678193476776205), (&#39;salary&#39;, 6.3746144901977475), (&#39;to_messages&#39;, 5.7652373136035786), (&#39;expenses&#39;, 4.2635766381444693), (&#39;from_this_person_to_poi&#39;, 3.0545709279872115), (&#39;deferred_income&#39;, 2.8591257010691469), (&#39;perc_to_poi&#39;, 1.5752718701560835), (&#39;from_messages&#39;, 1.3690711377259386), (&#39;shared_receipt_with_poi&#39;, 0.58945562335007018), (&#39;poi&#39;, 0.37046177768797534)]
{&#39;features&#39;: FeatureUnion(n_jobs=1,
       transformer_list=[(&#39;pca&#39;, PCA(copy=True, n_components=7, whiten=False)), (&#39;kbest&#39;, SelectKBest(k=6, score_func=&amp;lt;function f_classif at 0x000000001F36F128&amp;gt;))],
       transformer_weights=None), &#39;scaler&#39;: StandardScaler(copy=True, with_mean=True, with_std=True), &#39;features__pca__copy&#39;: True, &#39;svm__shrinking&#39;: True, &#39;svm__gamma&#39;: 1, &#39;svm__verbose&#39;: False, &#39;svm__probability&#39;: False, &#39;features__pca__whiten&#39;: False, &#39;features__kbest__k&#39;: 6, &#39;features__kbest__score_func&#39;: &amp;lt;function f_classif at 0x000000001F36F128&amp;gt;, &#39;svm__cache_size&#39;: 200, &#39;scaler__copy&#39;: True, &#39;svm__degree&#39;: 3, &#39;scaler__with_mean&#39;: True, &#39;features__kbest&#39;: SelectKBest(k=6, score_func=&amp;lt;function f_classif at 0x000000001F36F128&amp;gt;), &#39;svm__kernel&#39;: &#39;poly&#39;, &#39;svm__max_iter&#39;: -1, &#39;svm__coef0&#39;: 0.0, &#39;svm__random_state&#39;: None, &#39;scaler__with_std&#39;: True, &#39;svm&#39;: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=1,
  kernel=&#39;poly&#39;, max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False), &#39;features__pca__n_components&#39;: 7, &#39;svm__C&#39;: 1, &#39;svm__class_weight&#39;: None, &#39;svm__tol&#39;: 0.001, &#39;features__pca&#39;: PCA(copy=True, n_components=7, whiten=False)}
	Accuracy: 0.81413	Precision: 0.31398	Recall: 0.33250	F1: 0.32297	F2: 0.32862
	Total predictions: 15000	True positives:  665	False positives: 1453	False negatives: 1335	True negatives: 11547
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Conclusions and Reflection&lt;/h1&gt;

&lt;p&gt;In this analysis, we attempted to build a classification model that could predict whether someone is likely to be a Person of Interest (PoI) in the Enron scandal given their email and financial data. After exploring the data and removing outliers, we investigated two input featuresets: PCA transformed data and engineered features that provided a normalized representation of the email and financial data. Applying multiple classifier algorithms and tuning via GridSearch, the final model was developed which consisted of using a hybrid of the PCA and engineered featureset as the model inputs and a polynomial svm classifier with degree 3 and C and gamme values both 1. This final classifier was selected by reviewing the comparative performance of each models’ Precision and Recall scores, with the selected model having a Precision of 0.314 and Recall of 0.333. These performance metrics were chosen as they provide a balance between the goal of producing a short-list of potential PoI candidates to flag for further investigation, while preventing the over classification of non-PoI individuals. Using other performance metrics, such as accuracy in this situation would lead to sub-optimal classifiers, as the number of correct predictions is not as useful as ensuring people that are PoI’s are identified as such. This can be a common pitfall of performance measurement, and the goal of the model needs to be well defined before its performance can be measured.&lt;/p&gt;

&lt;p&gt;Using the GridSearch functionality of sklearn allowed for the automated tuning of the classifier algorithms. This tuning allowed for the use of different parameters in the classifiers, such as the type of svm (linear, rbf or polynomial) or the number of nearest neighbors to use in the KNeighbors classifier. This tuning is important in producing an effective classifier, as different datasets will result in different patterns. A linear svm may work well with clearly split datasets, but a polynimal svm will be better suited to datasets that result in curved patterns. Without correctly tuning the classification algorithms, a sufficient model will be difficult to develop.&lt;/p&gt;

&lt;p&gt;Although the final model meets the minimum goal of Precision and Recall &amp;gt;0.3, there is likely a more optimal classifier that could be developed by further exploring the dataset, engineering more intelligent features and investigating other classifier algorithms better suited to this problem. This model is also likely overfitted to this dataset and probably not directly useful in future investigations, but the proof of concept could be utilized by the investigators to generate their own model as they begin to identify PoI in their own investigation of a similiar nature to the Enron scandal. It would also be interesting to investigate if other features can be developed to help identify potential PoI, such as performing Natural Language Processing on the content of email messages to descern if any conversation patterns emerged between PoI. &lt;/p&gt;
</description>
        <pubDate>Sun, 08 Nov 2015 00:00:00 +0100</pubDate>
      </item>
    
      <item>
        <title>Fanduel MLB Exploratory Data Analysis</title>
        <link>http://cole-maclean.github.io//blog/Fanduel%20MLB%20Exploratory%20Data%20Analysis/</link>
        <guid isPermaLink="true">http://cole-maclean.github.io//blog/Fanduel%20MLB%20Exploratory%20Data%20Analysis/</guid>
        <description>&lt;h1&gt;
Introduction
&lt;/h1&gt;
&lt;p&gt;The Daily Fantasy Sports (DFS) industry has exploded in popularity in
recent years, largely due to the exponential growth of users playing on
industry titans such as Fanduel and DraftKings. These platforms allow
users to gamble real money by selecting a set of players known as a
roster from a sport, with rules constraining the total salary and
specific player position types required for a selected roster.Each
player in the roster can accumulate points based on their performance in
the sport in the upcoming game that day.The user then enters this roster
into contests against other users who have also entered their rosters,
with the aim of selecting the roster that accumulates the most points
based on the competitions set rule for point accumulation. The user or
subset of users with the top accumulated points at the end of the
competition win the pot of money entered by each user in the
competition. With the large amount of data now available for multiple
sports, and a platform to utilize that data on, this analysis attempts
to explore the potential predictive parameters available in generating a
successful roster for DFS competitions with the ultimate aim to project
roster points and select the optimal roster for a given competition. The
analysis is constrained to MLB competitions on Fanduel.&lt;/p&gt;

&lt;h2&gt;
Data Structure and Sources
&lt;/h2&gt;
&lt;p&gt;Data that will be used in this analysis comes from 3 sources:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Historical baseball game and player data -
&lt;a href=&quot;https://erikberg.com/api&quot;&gt;XMLStats&lt;/a&gt;  &lt;/li&gt;
  &lt;li&gt;Historical team lineup data - &lt;a href=&quot;http://www.baseballpress.com/lineups&quot;&gt;Baseball
Press&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Historical Weather Data -
&lt;a href=&quot;http://www.wunderground.com/weather/api&quot;&gt;Wunderground&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The data from the above sources has been munged and combined to produce
a structured data set for each player. The structures are split into two
categories representing the two major player types of baseball
competitions: batters and pitchers.Both batter and pitcher data is
organized into a data frame of the following structure:&lt;/p&gt;

&lt;p&gt;[Player GameID Player Stats Weather_Data Lineup_Data]&lt;/p&gt;

&lt;p&gt;Player and GameID makeup the unique data key for each record of the data
frame&lt;/p&gt;

&lt;p&gt;Player Stats data are individual data frame columns of each stat for the
player (ie. homeruns and hits for batters, era and strikeouts for
pitchers)&lt;/p&gt;

&lt;p&gt;Weather data contains a dataframe column “wunderground_forecast” that
contains a dictionarty of {‘wind’: {‘wind_speed’: 1.53, ‘wind_dir’:
206.67}, ‘temp’: 72.67, ‘humidity’: 19.33}&lt;/p&gt;

&lt;p&gt;LineupData contains 2 dataframe columns home_starting_lineup and
away_starting_lineups both of with contain dictionaries of {u’Hyun-Jin
Ryu_batter’: {‘batting_order’: ‘9’, ‘arm’: u’R’} for each player in
the starting lineup.&lt;/p&gt;

&lt;h2&gt;
Initial Investigation Plan
&lt;/h2&gt;
&lt;p&gt;The initial instinct is to jump right into combining various parameters,
plotting there relationships and reviewing their plots. However, a
clearer definition of the goal is required to better frame the direction
and good questions to ask of the Exploratory Data Analysis of the MLB
player dataset. The Fanduel scoring rules use the following functions to
calculate each players score:&lt;/p&gt;

&lt;p&gt;batters_FD_points =
singlesx1+doublesx2+triplesx3+home_runsx4+rbix1+runsx1+walksx1+stolen_basesx1+hit_by_pitchx1+(at_bats-hits])x-.25&lt;/p&gt;

&lt;p&gt;pitcher_FD_points =
winx4+earned_runsx-1+strike_outsx1+innings_pitchedx1&lt;/p&gt;

&lt;p&gt;Knowing this, there seems to be 2 approaches one could take in
attempting to develop a predictive model for projected player points:&lt;br /&gt;
1. Develop an analog function of fanduel scoring rules that attempts to
directly estimate projected player points with the models own internal
parameters.&lt;br /&gt;
2. Develop a model that projects the predicted value for each of the
parameters in the fanduel player point function and plugging them into
the function to calculate a players projected points.&lt;/p&gt;

&lt;p&gt;Knowing that the ultimate goal is to develop a predictive model and that
in doing so will require a choice between the above 2 options, this
analysis will help provide an exploration and give guidance on pathways
to begin developing the model with.&lt;/p&gt;

&lt;p&gt;We will first explore the univariate distributions of fanduel points,
batter and pitcher data and observe any interesting features in the
plots. We will then explore bivariate datasets to discover relationships
between parameters with a focus on their impact on fanduel points.
Lastly, we will further combine parameters into multivariable datasets
and try to discover any interacting relationships across parameters.
Using the patterns and relationships discovered in this analysis will
inform the development of a predictive model.&lt;/p&gt;

&lt;h2&gt;
Data Prep, Imports and Utility Functions
&lt;/h2&gt;
&lt;p&gt;All column histogram code from
&lt;a href=&quot;http://stackoverflow.com/questions/13035834/plot-every-column-in-a-data-frame-as-a-histogram-on-one-page-using-ggplot&quot;&gt;Stackoverflow&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;
Univarate Parameter Analysis
&lt;/h1&gt;
&lt;h2&gt;
Player Fanduel Points
&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/img/uni_FD_points-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -1.750  -0.250   0.250   1.529   2.500  27.750

## Standard Deviation =  2.695974
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/uni_FD_points-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -9.000   1.000   2.000   3.565   4.200  30.000

## Standard Deviation =  4.733862
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For batters, the distribution of FD_points seems to follow a decaying
distribution, while pitchers FD_points seem to be somewhat normally
distributed with a longer positive tail.This right-side-scew in the
batters distribution is quantified with the large distance between the
median and 3rd quartiles compared to the 1st quartile. The 1st quartile
is only 1/5th of a standard deviation from the median, whereas the 3rd
quartile is 1/3rd of a standard deviation from the median. Compared to
the more normally distributed pitchers distribution, where the 1st
quartile and 3rd quartile are 1/5th and 1/4th of a standard deviation
from the median respectively, quantifies it as being closer to a normal
distribution with a longer right-side tail. A repeating pattern in the
batter distribution that has a local maximum is likely due to the biases
created in the scaling of the batters scoring function. This information
might help in deciding to round model outputs to the discrete possible
ouputs of the scoring function. For both batters and pitchers, there is
a large number of 0 valued FD_points due to the inclusion of players
that did not participate in a game. This will need to be considered as
the dataset is explored further.&lt;/p&gt;

&lt;h2&gt;
Batters
&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/img/uni_batter_hist-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first observation from the above plots is the obvious discrete and
and low-variability nature of the data for many player stats. This
observation makes logical sense, as the game of baseball is made up of
many rules framed around discrete events. A player can run to only 1 of
4 bases, plate appearances are scheduled in the lineup order and there
is a limited subset of events that can occur in any one plate
appearance. There are some parameters, such as total bases, plate
appearances and the more advanced stats obp, slg, and ops that have a
less discrete nature and may warrant further exploration.&lt;/p&gt;

&lt;h2&gt;
Pitchers
&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/img/uni_pitcher_hist-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Many of the pitchers stats show the same characteristic of batters in
the discrete and low-variability nature. The pitching stats count and
strikes have interesting double hump distributions that may be
attributed to a select few pitchers that usually pitch a full 8 or 9
innings.&lt;/p&gt;

&lt;p&gt;The discrete nature of the parameters that are used in the Fanduel
scoring functions leads me to believe it will be difficult to accurately
develop a model based on predicting each individual input parameter. For
example, the difference between 0 and 2 runs is small, but the impact on
point projections and ultimately selecting the optimum roster could be
large. The distribution of FD_points more readily lends itself to
regression analysis, suggesting that developing an analog function
predicting player fanduel points may produce better results than
directly predicting the input parameters. Alternatively, it may be
possible to develop a hybrid of the two options, using classification of
the discrete inputs as features in the regression model.&lt;/p&gt;

&lt;h1&gt;
Bivarate Parameter Analysis
&lt;/h1&gt;
&lt;p&gt;We know that the player statistics included in fanduel’s scoring
function will have a linear relationship with fanduel points, as
dictated by the form of fanduel’s scoring equations. For example,
batters rbi’s and pitchers strikeouts. Code adapted from
&lt;a href=&quot;http://www.cookbook-r.com/Graphs/Scatterplots_(ggplot2)&quot;&gt;R-Cookbook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/bi_batter-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## r2 =  0.9910313
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/bi_batter-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## r2 =  0.9967708
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The strong linear relationships, with r2’s of 0.99 for both rbi and
strike_outs is obvious and not very informative, as the linear
relationship of these parameters is built into the fanduel scoring
equations, and by definition are linear relationships of fanduel points.
As was discussed above, it will likely be difficult to accurately
predict the individual parameters in fanduel’s point function. What is
of more interest will be the relationships between the more advanced
player statistics and FD points.&lt;/p&gt;

&lt;h2&gt;
Advanced Stats
&lt;/h2&gt;
&lt;h3&gt;
Batters
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/bi_batter_adv-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## r2 =  0.7054033
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/bi_batter_adv-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## r2 =  0.7736614
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/bi_batter_adv-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## r2 =  0.7834183
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It appears the advanced stats for batters may be following a more
quadratic or exponential relationship then linear, with the linear
regression r2 values being 0.70, 0.77 and 0.78 for obp,slg and ops
respectively. I had originally guessed the relationships would have very
strong linear relationships with r2’s closer to 1, so it is good to
discover something counter thesis to my intuition in this data
exploration. The fact that the relationships seem quite similar is
expected, as the advanced stats are closely related.&lt;/p&gt;

&lt;p&gt;Cleaning up some of the outliers and applying a regression to the
batters slg vs FD_points stats seems like a good candidate for closer
inspection in the more detailed section of this analysis.&lt;/p&gt;

&lt;h3&gt;
Pitchers
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/bi_pitcher_adv-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## r2 =  0.07168485
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only advanced stat for pitchers in this dataset is their WHIP. A
clear pattern is hard to determine from this plot, with a potential
linear or logarithmic relationship that has a strong clustering in the
bottom left quadrant of the graph. The low rsquared linear regression
value of 0.07 shows that building a linear model with the WHIP parameter
in it’s current state will not be very useful, and other data
transformations may be required. The large variation in the scatter of
this relationship may mean that whip will not provide usable information
in creating a predictive model, but it may be possible to combine whip
with another feature to give it stronger predictive power. One option
could be to find the characteristic(s) of a pitcher that causes him to
cluster into the lower left quadrant of this plot. We could then run a
clustering classifier on pitchers, that produces as a result the pitcher
either does or doesn’t clump into the lower left quadrant. If he does,
we assign him a default prediction value (ie the mean of the cluster).
If he doesn’t, we can then apply the linear/logarithmic function as a
prediction for the players that do not seem to cluster. This will
require further exploration in the modelling stage.&lt;/p&gt;

&lt;h2&gt;
Weather
&lt;/h2&gt;
&lt;p&gt;As some MLB baseball diamonds are outdoors, the weather may have an
impact on player performance. The below plots investigate temperature
and humidity for batters and pitchers. Wind will be investigated in the
multi-variate analysis, as direction and speed will come into play. The
decode_wg_weather utility function is used to parse out open air
stadiums only and decode the wunderground weather json data.&lt;/p&gt;

&lt;h3&gt;
Batters
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/bi_batter_weather-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This plot is clearly not very useful, but does show that the wundeground
weather temperature data contains some wonky data with large negative
values seen in the dataset. Cleaning up these outliers and 0 valued
temps and moving to box plots:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;batter_data.OD.decoded$bin_temp &amp;lt;- cut(batter_data.OD.decoded$temp,
                                       c(25,35,45,55,65,75,85,95,105))
temp_groups &amp;lt;- group_by(batter_data.OD.decoded,bin_temp)
batter_data.temp_meds &amp;lt;- summarize(temp_groups,
                                   FD_points_median = median(FD_points))
ggplot(batter_data.OD.decoded, aes(x=bin_temp, y=FD_points)) +
  coord_cartesian(ylim = c(-2, 10)) +
  geom_boxplot() +
  geom_text(data = batter_data.temp_meds,
            aes(x = bin_temp, y = FD_points_median,
            label = FD_points_median), 
            size = 3, vjust = -1.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/bi_batter_temp-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s difficult to say if temperature has any affect at all on a batters
FD points, with median values being quite similar across all temperature
bins. There may be a slightly quadratic relationship with the median
values being higher at the cold and hot edges of the temperature
distribution, but further work will be required to prove if this is
useful for the predictive model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/bi_batter_humd-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, humidity seems to have very low impact on a batters FD_points
with median values being very similar across all humidity bins and no
desernable pattern in the boxplots.&lt;/p&gt;

&lt;h3&gt;
Pitchers
&lt;/h3&gt;
&lt;p&gt;Pitchers may be more affected by weather then batters.
&lt;img src=&quot;/img/bi_pitcher_temp-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With a median of 2 FD_points for all temp bins, the lack of a clear
trend in FD_points vs temp for pitchers matches what was seen for
batters and does not seem to be very as a predictive parameter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/bi_pitcher_humd-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Humidity also seems to provide little predictive power for pitchers
FD_points, with medians of 2 persisting in each humidity bin.&lt;/p&gt;

&lt;p&gt;It does not appear that weather (temp and humidity) have a very big
impact on a MLB players FD_points. This data may be useful to create a
classifier that determines if a game will be rained out, as selecting
players from a postponed game will greatly affect the roster performance
(ie. selected a player that ends up with 0 FD_points) and may need to
be explored further during model development.&lt;/p&gt;

&lt;h1&gt;
Multi-Variate Analysis
&lt;/h1&gt;
&lt;p&gt;There may exist interactions between parameters that reveal interesting
relationships we can take advantage of for model development.&lt;/p&gt;

&lt;h2&gt;
Wind
&lt;/h2&gt;
&lt;p&gt;The first multi-variate parameter investigated will be the impact of
wind speed and direction on players FD_points. The function
wind_spiral_plots is defined to parse the data into 6 windspeed bins,
calculate the mean FD_points for each wind_angle per windspeed bin and
plots the spiral plot of angle vs mean FD_points for all 6 windspeed
bins. There likely exists a more elegant solution then the current
functions code, but the functionality is adequate. I’ll need to sharpen
my R coding skills and come back to refactor this function at a later
date.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Function to parse dataframe wind data and 
#create spiral plots based on wind angle and FD_points.
wind_spiral_plot &amp;lt;-function(df){
  wind_speed_bins = c(-Inf,4,6,8,10,12,Inf) 
  df$bin_wind_speed = cut(df$wind_speed,wind_speed_bins) #Bin wind speeds
  df.wind_speed_groups = split(df,f=df$bin_wind_speed)
  wind_df = data.frame()
  #Loop through each wind speed dataframe
  for(i in 1:length(df.wind_speed_groups)) 
    {
      wind_dir_groups &amp;lt;- with(df.wind_speed_groups[[i]],
                      cut(wind_dir,c(0,25,50,75,100,125,150,175,
                                     200,225,250,275,300,325,359),
                          include.lowest = TRUE))
    dat &amp;lt;- aggregate(FD_points ~ wind_dir_groups,
                     data = df.wind_speed_groups[[i]], FUN = mean)
                    #Summarize FD_points per wind angle
    dat$wind_speed_group_indx = wind_speed_bins[i]
    wind_df&amp;lt;-rbind(wind_df,dat)
    }
  return (ggplot(wind_df,aes(x = wind_dir_groups, y = FD_points)) + 
      geom_bar(stat=&#39;identity&#39;) + coord_polar(start = 0) +
      facet_wrap(~wind_speed_group_indx))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;
Batters
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/batter_wind-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There appears to be some effect of wind speed and direction on batters
fanduel points that may prove useful as a feature for model development.
Some wind speeds seem to have spikes in FD_points in the upper left
quadrant, while others show spikes at many angles. More work will need
to be done to prove the significant of a relationship, as well a doing a
more detailed investigation to analyze the effect of wind for individual
ballparks. This will be looked at in the more detailed section of this
analysis.&lt;/p&gt;

&lt;h3&gt;
Pitchers
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/pitcher_wind-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pitchers do not seem to be as affected by wind speed and direction as
batters, with few obvious spikes or variations in FD_points at
different speeds and angles. Wind data may not prove to be an
informative parameter for the predictive model, but a more granular
investigation at a per ballpark level should be conducted.&lt;/p&gt;

&lt;h1&gt;
Detailed Plots and Analysis
&lt;/h1&gt;
&lt;p&gt;Three plots have been identified that warrant further investigation:&lt;br /&gt;
1. Batter FD_points vs SLG to test regression models&lt;br /&gt;
2. Pitcher FD_points vs WHIP to better identify clustering and trend in
the relationship&lt;br /&gt;
3. Batter wind data spiral plots to better understand the significance
and strength of the effect wind has on batter performance&lt;/p&gt;

&lt;h2&gt;
Batter FD\_point vs SLG
&lt;/h2&gt;
&lt;p&gt;Batter FD_points vs SLG was chosen in the final plots section as it
displayed a potentially quadratic or exponential relationship, that if
properly defined, could prove useful in building a predictive model.&lt;/p&gt;

&lt;p&gt;Code adapted from
&lt;a href=&quot;http://stats.stackexchange.com/questions/64927/how-do-i-plot-an-exponential-curve-in-r&quot;&gt;Stackoverflow&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;batter_data.adv_stats_mean.clean = subset(batter_data.adv_stats_mean,
                                          isnumeric(slg_mean) &amp;amp; slg_mean &amp;gt;
                                            0 &amp;amp; slg_mean &amp;lt; 0.7)
                                            #clean up NA and outlier values
y = batter_data.adv_stats_mean.clean$FD_points_mean
x = batter_data.adv_stats_mean.clean$slg_mean
xy = batter_data.adv_stats_mean.clean
linear &amp;lt;- lm(y ~ x, data = xy)
quadratic &amp;lt;- lm(y ~ I(x^2), data = xy)
exponential &amp;lt;- lm(y~I(exp(x)),data=xy)
linear_r2_text = paste(&quot;lin_&quot;,
                 toString(round(summary(linear)$r.squared,digits=2)))
quadratic_r2_text = paste(&quot;quad_&quot;,
                    toString(round(summary(quadratic)$r.squared,digits=2)))
exponential_r2_text = paste(&quot;exp_&quot;,
                      toString(round(summary(exponential)$r.squared,digits=2)))
r2_text = paste(quadratic_r2_text,exponential_r2_text,linear_r2_text,sep=&quot;\n&quot;)
prd &amp;lt;- data.frame(x = seq(0, 0.7, by = 0.05))
result &amp;lt;- prd
result$linear &amp;lt;- predict(linear, newdata = prd)
result$quadratic &amp;lt;- predict(quadratic, newdata = prd)
result$exponential &amp;lt;- predict(exponential, newdata = prd)
result &amp;lt;-  melt(result, id.vars = &quot;x&quot;, variable.name = &quot;model&quot;,
                value.name = &quot;fitted&quot;)
ggplot(result, aes(x = x, y = fitted)) +
  theme_bw() +
  geom_point(data = xy, aes(x = x, y = y)) +
  geom_line(aes(colour = model), size = 1) +
  annotate(&quot;text&quot;, x = 0.65, y = 3.5, label = r2_text) +
  xlab(&quot;Batter SLG %&quot;) +
  ylab(&quot;Fanduel Points&quot;) +
  ggtitle(&quot;Batter Fanduel Points vs SLG %&quot;) +
  ylim(c(-1,4))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/final_slg-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The cleaned up version of batter FD_points vs SLG plot shows that the
initial impression of a highly quadratic or exponential relationship may
not have been accurate. The R2 for a linear model is nearly identical to
that of a quadratic, with an exponential regression being the worst r2
of the models. More advance outlier cleaning techniques may be required
during model development.&lt;/p&gt;

&lt;h2&gt;
Pitcher FD\_points vs WHIP
&lt;/h2&gt;
&lt;p&gt;Pitcher FD_points vs WHIP was chosen to include in the final plots
section, as WHIP is currently the only advanced stat for pitchers in the
dataset and showed an interesting distribution in the scatterplot, that
if properly classified into clusters could provide useful for grouping
pitchers into fanduel point ranges for use in a predictive model.&lt;/p&gt;

&lt;p&gt;Circle function from
&lt;a href=&quot;http://stackoverflow.com/questions/6862742/draw-a-circle-with-ggplot2&quot;&gt;Stackoverflow&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;circleFun &amp;lt;- function(center = c(0,0),diameter = 1, npoints = 100){
    r = diameter / 2
    tt &amp;lt;- seq(0,2*pi,length.out = npoints)
    xx &amp;lt;- center[1] + r * cos(tt)
    yy &amp;lt;- center[2] + r * sin(tt)
    return(data.frame(x = xx, y = yy))
}
pitcher_data.adv_stats_mean.clean = subset(pitcher_data.adv_stats_mean,
                                           isnumeric(whip_mean) &amp;amp; whip_mean &amp;gt;
                                             0 &amp;amp; whip_mean &amp;lt;=5)
                                          #clean up NA and outlier values
dat &amp;lt;- circleFun(c(1.4,1.38),1.6,npoints = 100)
ggplot(pitcher_data.adv_stats_mean.clean, aes(x=whip_mean, y=FD_points_mean)) +
  geom_point(shape=1) +
  geom_path(data=dat,aes(x,y),color=&quot;red&quot;,size=2) +
  xlab(&quot;Pitcher WHIP&quot;) +
  ylab(&quot;Fanduel Points&quot;) +
  ggtitle(&quot;Pitcher Fanduel points vs WHIP&quot;) +
  ylim(c(-5,15))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/final_whip-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cleaning up the pitcher WHIP data for outliers and plotting the results
magnifies the clustering that occurs at the lower left of the dataset,
and also shows a possible logarithmic trend for data points outside the
cluster. It may be possible for the model to develop a automated cluster
classification and apply a logarithmic regression to pitchers that fall
outside this cluster as part of the predictive model.&lt;/p&gt;

&lt;h2&gt;
Batter Wind Effects
&lt;/h2&gt;
&lt;p&gt;Batter wind effects were chosen for inclusion in the final plots section
as some variation in fanduel points vs windspeed and direction were seen
in the plots. This advanced relationship could prove to be an edge in
predictive models over competing models that not account for the
potential wind relationship in predicting batter fanduel points.&lt;/p&gt;

&lt;p&gt;Utilizing the wind_spiral_plot function defined earlier and refining:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wind_spiral_plot_formatted &amp;lt;-function(df){
  wind_spiral_plot(df) +
  aes(fill=wind_speed_group_indx) + 
  guides(fill=guide_legend(title=&quot;Wind Speed m/s&quot;)) + 
  ggtitle(&quot;Batter Fanduel points vs Wind speed and Direction&quot;) +
  scale_x_discrete(breaks=c(&quot;(75,100]&quot;,&quot;(150,175]&quot;,&quot;(275,300]&quot;,
                            &quot;(325,359]&quot;),
                   labels=c(&quot;(75,100]&quot;=&quot;75&quot;,&quot;(150,175]&quot;=&quot;150&quot;,
                                             &quot;(275,300]&quot;=&quot;275&quot;,
                                             &quot;(325,359]&quot;=&quot;325&quot;)) +
  theme(strip.background = element_blank(),strip.text.x = element_blank()) +
  xlab(&quot;&quot;) +
  ylab(&quot;Fanduel Points&quot;)
}
wind_spiral_plot_formatted(batter_data.OD.decoded)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/img/final_wind-1.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are some large outliers in FD_points at windspeeds of 2m/s, 10m/s
and 12m/s, which may prove useful in a predictive model.Further analysis
is needed to explore each ballpark and it may be beneficial to normalize
the dataset to ballpark homeplate direction, but the inclusion of wind
as an advanced parameter in the predictive model may prove to be an
advantage of simpler competing models.&lt;/p&gt;

&lt;h1&gt;
Reflection
&lt;/h1&gt;
&lt;p&gt;Being a dynamic sport, with so many variables and events, parsing and
exploring the MLB dataset proved both challenging an insightful. The
biggest challenges in this analysis included manipulating data from
multiple sources and formats, and coercing that data into useful and
interpretable visualizations. Coming up with an idea of a beautiful plot
is the easy part; mangling the data and actually building the plot takes
effort and many iterations to obtain a refined product. Two specific
challenges included unraveling the json wunderground weather dataset
into usable data for plotting. This took some advanced R decoding
functions and long hours of debugging multiple errors. Another challenge
was in learning R-programming syntax in general, especially ggplot2 use
cases. Many hours were spent goggling different methods in attempts to
create the plots shown above, and many others not shown that did not
produce the expected result. After a lot of trial and error, successful
methods of generating the desired plots were found. As the analysis
progressed, the number of trials before success decreased, as I began to
get a more intuitive understanding of the R and ggplot2 syntax. The
ability to continuously add layers to a plot is very cool, and I’m
especially proud of the wind_spiral_plot_formatted which takes the
returned ggplot of the wind_spiral_plot and expands on it to refine
the plot. I can imagine there being a lot of interesting use cases with
this kind of functionality.&lt;/p&gt;

&lt;p&gt;This analysis provides many useful insights for a starting point to
develop a predictive model. It provided support in the decision to
develop an analog model of FD_points instead of attempting to predict
the FD scoring function inputs directly. It showed both advanced stats
and wind for batters seem informative, and displayed paths that may not
be valuable to pursue, such as wind parameters for pitchers. Being as
dynamic as it is, there are many, many more parameters and relationships
to be explored for building a MLB FD_points predictive model. The most
obvious omission in the above analysis is its lack of investigation into
the impact of oppositional teams on a players FD_points. For example,
exploring a batters points vs a low WHIP pitcher, or a pitchers points
versus a high scoring team. These relationships will need to be explored
before a optimal predictive model can be develop, but will require
further data munging to align the dataset for that sort of analysis.
That will likely be the subject of a future analysis.&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Oct 2015 00:00:00 +0200</pubDate>
      </item>
    
      <item>
        <title>The Diagonality of Barcelona</title>
        <link>http://cole-maclean.github.io//blog/The%20Diagonality%20of%20Barcelona/</link>
        <guid isPermaLink="true">http://cole-maclean.github.io//blog/The%20Diagonality%20of%20Barcelona/</guid>
        <description>&lt;h2&gt;Data Munging and Analyzing Barcelona OSM Data&lt;/h2&gt;

&lt;p&gt;The XML data for the city boundary of Barcelona was downloaded from OSM to clean and transform into a json encodable structure to allow loading into MongoDB, providing storage and artbitrary querying to enable further data analysis of the Barcelona OSM dataset. I chose the Barcelona dataset because I just recently spent a few weeks there and noticed some pretty cool urban planning features of the city (ie. Avinguda Diagonal) that could be cool to explore further.&lt;/p&gt;

&lt;p&gt;Inspired by the “Diagonality” of Barcelona streets and to give the scope of the project a little more focus, I will be attempting to analyze the Barcelona OSM data and see if I can generate a measure for the “degree of diagionality” for Barcelona and possibly compare this measure with other cities. Not knowing ahead of time the difficulty of this, I may eventually have to delete this previous sentence, but I’m interested enough to give it a shot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static1.squarespace.com/static/52b3aae6e4b00492bb71aa3d/t/54776cbde4b019f8929d0684/1414769949925/?format=1500w&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;http://www.dailyoverview.com/six/&quot;&gt;dailyoverview.com&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Imports and Data prep&lt;/h2&gt;

&lt;p&gt;```python
%matplotlib inline
import xml.etree.cElementTree as ET
import pprint
import re
import codecs
import json
import math
import numpy as np
import matplotlib.pyplot as plt
import heapq&lt;/p&gt;

&lt;p&gt;OSM_FILE = “C:/Users/Cole/Desktop/Udacity/Data Analyst Nano Degree/Project 3/barcelona_spain.osm”  # Replace this with your osm file
SAMPLE_FILE = “C:/Users/Cole/Desktop/Udacity/Data Analyst Nano Degree/Project 3/barcelona_spain_sample.osm”
def get_element(osm_file, tags=(‘node’, ‘way’, ‘relation’)):
    “&quot;”Yield element if it is the right type of tag&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference:
http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python
&quot;&quot;&quot;
context = ET.iterparse(osm_file, events=(&#39;start&#39;, &#39;end&#39;))
_, root = next(context)
for event, elem in context:
    if event == &#39;end&#39; and elem.tag in tags:
        yield elem
        root.clear()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;with open(SAMPLE_FILE, ‘wb’) as output:
    output.write(‘&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n’)
    output.write(‘&lt;osm&gt;\n  &#39;)&lt;/osm&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Write every 100th top level element
for i, element in enumerate(get_element(OSM_FILE)):
    if i % 100 == 0:
        output.write(ET.tostring(element, encoding=&#39;utf-8&#39;))

output.write(&#39;&amp;lt;/osm&amp;gt;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Visual Data Inspection&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
#This code attempts to pick out unique data structures from the SAMPLE_FILE to give us examples of 
unique_elements = []
for i,element in enumerate(get_element(SAMPLE_FILE)):
    elem_children = [element.tag]
    for child in element.iter():
        elem_children.append(child.tag)
    if elem_children not in unique_elements:
        unique_elements.append(elem_children)
        #print ET.tostring(element, encoding=&#39;utf-8&#39;)
print (len(unique_elements))
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Reviewing the unique elements above yields a few considerations for potential data issues and facts pertinent to the goal of reporting a “degree of diagionality”:&lt;br /&gt;
1. Being a Spanish City, the non-standard characters common in Spanish names might exist that could cause potential problems 
2. The amount of data for a given node or way varies. The minimum required data to enable calculation of a streets angle will need to be defined.
3. Nodes contain the required lat/lon coordinates required for calculating a ways angle&lt;br /&gt;
4. Ways list the nodes that create the path of a street. This path may not be a straight line, which may have an impact on the angle calculation&lt;br /&gt;
5. Not all ways have a street attribute, but still seem valid, such as examples that have a “highway” attribute&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Data Shaping&lt;/b&gt;&lt;br /&gt;
In order to perform a calculation of “diagonality” of a way, we need at least 2 nodes on the way that have lat/lon data. To ensure the way is a valid road to calculate diagionality on, the way must have one of: a valid steet or highway attribute. We will only keep top level data such as id, lat/lon and created data for nodes. To load this data into MongoDB for storage and querying, we need to shape this data into valid JSON, which the below code does.&lt;/p&gt;

&lt;p&gt;```python
#Utility Regular Expressions
lower = re.compile(r’^([a-z]|&lt;em&gt;)*$’)
lower_colon = re.compile(r’^([a-z]|&lt;/em&gt;)&lt;em&gt;:([a-z]|_)&lt;/em&gt;$’)
problemchars = re.compile(r’[=+/&amp;amp;&amp;lt;&amp;gt;;&#39;”\?%#$@\,. \t\r\n]’)&lt;/p&gt;

&lt;h1 id=&quot;utility-data-structures-for-nested-dictslists&quot;&gt;Utility data structures for nested dicts/lists&lt;/h1&gt;
&lt;p&gt;TOP_LEVEL_DATA = [‘id’,’visible’]
CREATED = [ “version”, “changeset”, “timestamp”, “user”, “uid”]
POS=[‘lat’,’lon’]&lt;/p&gt;

&lt;h1 id=&quot;this-function-returns-a-data-dictionary-by-parsing-the-data-from-each-key-of-an-elements-attributes&quot;&gt;This function returns a data dictionary by parsing the data from each key of an elements attributes&lt;/h1&gt;
&lt;p&gt;#listed in the data_list argument
def data_dict(elem_attribs,data_list):
    return {data_elem:elem_attribs[data_elem] for data_elem in data_list if data_elem in elem_attribs.keys()} &lt;/p&gt;

&lt;p&gt;def extract_top_level_data(element): # this function extract top level data from the elements attributes
    node = data_dict(element.attrib,TOP_LEVEL_DATA)
    created_dict = data_dict(element.attrib,CREATED)
    if created_dict:
        node[‘created’] = created_dict
    node[‘type’] = element.tag
    return node&lt;/p&gt;

&lt;p&gt;def shape_element(element):
    node_refs = []
    if element.tag == “node”: #Checks if is node and extracts required data
       node = extract_top_level_data(element)
       for child in element.iter():
            pos_list = [float(child.attrib[pos_data]) for pos_data in POS if pos_data in child.attrib.keys()] #extracts lat/lon data in list and converts to float
            if pos_list:
                node[‘pos’] = pos_list
       return node
    elif element.tag == “way”: #Checks if is way and extracts required data
        node = extract_top_level_data(element)
        for child in element.iter():
            if child.tag ==’tag’:
                m = re.search(problemchars,child.attrib[‘k’]) #searches for problematic characters and returns None if encountered
                if m:
                    return None
                if child.attrib[‘k’] ==”addr:street”:#Checks if way has addr:street attribute and assigns the ‘street’ key the streets name, otherwise ‘highway’
                    node[‘street’] = child.attrib[‘v’]
                elif “highway” in child.attrib.values() and ‘street’ not in node.keys():
                    node[‘street’] = child.attrib[‘k’]
            if child.tag == ‘nd’:
                node_refs.append(child.attrib[‘ref’])
        if node_refs:
            node[“node_refs”] = node_refs
        else:
            return None
        if ‘street’ in node.keys() and len(node[‘node_refs’])&amp;gt;=2: #Only return ways that have a addr:street or highway attribute and 2 or more node refs
            return node
        else:
            return None
    else:
        return None&lt;/p&gt;

&lt;p&gt;def process_map(file_in, pretty = False):
    # You do not need to change this file
    file_out = “{0}.json”.format(file_in)
    data = []
    with codecs.open(file_out, “w”) as fo:
        for _, element in ET.iterparse(file_in):
            el = shape_element(element)
            if el:
                data.append(el)
        if pretty:
            fo.write(json.dumps(data, indent=2)+”\n”)
        else:
            fo.write(json.dumps(data) + “\n”)
    return data&lt;/p&gt;

&lt;p&gt;process_map(OSM_FILE)
print “done”
```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;done
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Data Analysis&lt;/h2&gt;
&lt;p&gt;Now that the data has been cleaned and converted to json, it is able to be stored into a MongoDB database and queried for analysis. Although we’ll use the full dataset that was cleaned and stored in analyzing the “degree of diagonality” of Barcelona, thus not requiring custom MongoDB queries, an example of a query if we were interesting in only ways that are highways would be:&lt;br /&gt;
query_results = db.barca_OSM.find({“street”:”highway”})&lt;/p&gt;

&lt;p&gt;```python
#Reference: Code from https://gist.github.com/jeromer/2005586
def calculate_initial_compass_bearing(pointA, pointB):
    “””
    Calculates the bearing between two points.
    The formulae used is the following:
        θ = atan2(sin(Δlong).cos(lat2),
                  cos(lat1).sin(lat2) − sin(lat1).cos(lat2).cos(Δlong))
    :Parameters:
      - &lt;code&gt;pointA: The tuple representing the latitude/longitude for the
        first point. Latitude and longitude must be in decimal degrees
      - &lt;/code&gt;pointB: The tuple representing the latitude/longitude for the
        second point. Latitude and longitude must be in decimal degrees
    :Returns:
      The bearing in degrees
    :Returns Type:
      float
    “””
    if (type(pointA) != tuple) or (type(pointB) != tuple):
        raise TypeError(“Only tuples are supported as arguments”)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lat1 = math.radians(pointA[0])
lat2 = math.radians(pointB[0])

diffLong = math.radians(pointB[1] - pointA[1])

x = math.sin(diffLong) * math.cos(lat2)
y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1)
        * math.cos(lat2) * math.cos(diffLong))

initial_bearing = math.atan2(x, y)

# Now we have the initial bearing but math.atan2 return values
# from -180° to + 180° which is not what we want for a compass bearing
# The solution is to normalize the initial bearing as shown below
initial_bearing = math.degrees(initial_bearing)
compass_bearing = (initial_bearing + 360) % 360

return compass_bearing ```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```python
degree_data = []
with open(“C:/Users/Cole/Desktop/Udacity/Data Analyst Nano Degree/Project 3/barcelona_spain.osm.json”,”r”) as data_file:
    data = json.load(data_file)
node_pos = {datum[‘id’]:datum[‘pos’] for i,datum in enumerate(data) if datum[‘type’]==’node’}
for indx,way_data in enumerate(data):
    if way_data[‘type’] == ‘way’:
        try:
            node_1_pos = tuple(node_pos[way_data[‘node_refs’][0]])
            node_2_pos = tuple(node_pos[way_data[‘node_refs’][1]])
            degree_data.append(calculate_initial_compass_bearing(node_1_pos,node_2_pos))
        except KeyError:
            pass
print degree_data[3:8]&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[267.85381820650105, 136.77552225283853, 225.17064639252413, 225.77167855473186, 169.69113420727763]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Data Visualization&lt;/h2&gt;
&lt;p&gt;With the street angle data, we can now generate a circular historgram plot across the street angles and visualize the most dominate directions of Barcelona streets.
Code adapted from &lt;a href=&quot;http://stackoverflow.com/questions/22562364/circular-histogram-for-python&quot;&gt;SO Circular Histogram for Python&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;```python
fig = plt.figure(figsize=(10, 12), dpi=100)
N = 36
hist, edges = np.histogram(degree_data, bins=N)
bottom = 8&lt;/p&gt;

&lt;p&gt;theta = np.delete(edges,-1)
radii = hist
width = (2*np.pi) / N&lt;/p&gt;

&lt;p&gt;ax = plt.subplot(111, polar=True)
bars = ax.bar(theta, radii, width=width, bottom=bottom)&lt;/p&gt;

&lt;h1 id=&quot;use-custom-colors-and-opacity&quot;&gt;Use custom colors and opacity&lt;/h1&gt;
&lt;p&gt;for r, bar in zip(radii, bars):
    bar.set_facecolor(plt.cm.jet(r / 10.))
    bar.set_alpha(0.8)
plt.show()
```&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/output_12_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Final Analysis&lt;/h2&gt;
&lt;p&gt;It appears there are 3 dominate street angles at around 135o, 255o and 5o. Using this visualization as a guide, we could develop a measure of diagonality as defined by taking 1 minus the count of the 2 most dominate angle’s and dividing that by the total count of the dataset. A perfectly non-diagonal city would result in a diagonality measure of (1-Count(dominant_angles))/(total_angles) = 0 where the 2 dominant angles are the only angles = total angles&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
dominant_angles = np.sum(heapq.nlargest(2, hist))
total_angles = np.sum(hist)
print &quot;barcelona degree of diagonality =&quot; + str(1-(float(dominant_angles)/float(total_angles)))
&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;barcelona degree of diagonality =0.895038509762
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Final Conclusions&lt;/h2&gt;
&lt;p&gt;The “degree of diagonality” of Barcelona using the above measure is 0.895. I intuitivley guess this is a relatively (globally speaking) high degree of diagonality, but other cities could be run through the above methodology and compared. I had alot of fun with the above analysis, and can think of many further areas to continue developing and errors to fix, but I have a Nano Degree to finish!&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0200</pubDate>
      </item>
    
  </channel>
</rss>
