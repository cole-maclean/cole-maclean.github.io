<!DOCTYPE html>
<!--[if IE 6]>
<html id="ie6" lang="zh-CN">
<![endif]-->
<!--[if IE 7]>
<html id="ie7" lang="zh-CN">
<![endif]-->
<!--[if IE 8]>
<html id="ie8" lang="zh-CN">
<![endif]-->
<!--[if !(IE 6) | !(IE 7) | !(IE 8)  ]><!-->
<html lang="en-US" class="js">
<!--<![endif]-->
  

	
	    
	        
	    
	        
	    
	        
	    
	    
	

	
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	
<head>
	<meta charset="utf-8">
	<title>Recurrent Neural Network Based Subreddit Recommender System | Online Profile</title>
	<meta name="description" content=", python, tensorflow, rnn, bokeh, EDA, Data Munging, Deep Learning, Recommender Systems, IntroductionAs part of a project course in my second semester, we were tasked with building a system of our chosing that encorporated or showcased any of the...">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">
	<!-- CSS -->
	<link rel="stylesheet" href="/css/main.css">
	<link rel='stylesheet' id='divi-style-css'  href="/css/style.css" type='text/css' media='all' />
	<link rel="stylesheet" href="/css/post.style.css">
	<link rel="stylesheet" href="/css/highlight.css">

	<script type='text/javascript' src="/js/jquery.js"></script>
	<script type='text/javascript' src="/js/soundmanager2-nodebug-jsmin.js"></script>

	<!--Favicon-->
	<link rel="icon" href="favicon.ico" type="image/x-icon"/>
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	
	<link href="/favicon.ico" mce_href="favicon.ico" rel="bookmark" type="image/x-icon" /> 
	<link href="/favicon.ico" mce_href="favicon.ico" rel="icon" type="image/x-icon" /> 
	<link href="/favicon.ico" mce_href="favicon.ico" rel="shortcut icon" type="image/x-icon" /> 
	<!-- Canonical -->
	<link rel="canonical" href="http://cole-maclean.github.io//blog/RNN-Based-Subreddit-Recommender-System/">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Online Profile" href="http://cole-maclean.github.io//feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.3.0/katex.min.css">
	<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.3.0/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-71787228-1', 'auto');
	ga('send', 'pageview');
	</script>
	

	<script type="text/javascript">
    var host = "http://cole-maclean.github.io/";
    if ((host == window.location.host) && (window.location.protocol != "https:"))
        window.location.protocol = "https";
    </script>
     <!-- Google Analytics: change UA-XXXXX-X to be your site's ID. -->
<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-71787228-1', 'auto');
ga('send', 'pageview');

</script>
 
</head>

	<body class="home blog et_pb_button_helper_class home-posts et_fixed_nav et_show_nav et_cover_background et_pb_gutter windows et_pb_gutters3 et_primary_nav_dropdown_animation_fade et_secondary_nav_dropdown_animation_fade et_pb_footer_columns4 et_header_style_left et_right_sidebar chrome">
		<div id="page-container">
    		<header id="main-header" data-height-onload="66">
	<div class="container clearfix et_menu_container">
		<div class="logo_container">
			<span class="logo_helper"></span>
			<a href="/">
				<!--img src="http://blog.ibireme.com/wp-content/uploads/2015/05/logo1.png" alt="Garan no dou" id="logo" data-height-percentage="54" /-->
				Online Profile - Cole MacLean
			</a>
		</div>
		<div id="et-top-navigation" data-height="66" data-fixed-height="40">
			<nav id="top-menu-nav">
				<ul id="top-menu" class="nav">
					<li>
						<a class="page-link" href="/">
							Home
						</a>
					</li>
					
					
					<li>
						<a class="page-link" href="/projects/">
							Projects
						</a>
					</li>
					
					
					
					<li>
						<a class="page-link" href="/About/">
							About
						</a>
					</li>
					
					
					
					
					
					
					
					<li>
						<a class="page-link" href="/Archives/">
							Archive
						</a>
					</li>
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					
					<!-- Social icons from Font Awesome, if enabled -->
					<li>
						<a class="page-link" href="">Find Me</a>
						<ul class="children">












<li>
	<a href="https://github.com/cole-maclean" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>









<li>
	<a href="https://www.linkedin.com/in/macleancole" title="Follow on LinkedIn">
		<i class="fa fa-fw fa-linkedin"></i>
	</a>
</li>















<li>
	<a href="https://twitter.com/ponderingHplus" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>







<li>
	<a href="https://www.kaggle.com/colemaclean" title="">
		<i>kaggle</i>
	</a>
</li>



<li>
	<a href="mailto:maclean.cole@gmail.com" title="Email">
		<i class="fa fa-fw fa-envelope"></i>
	</a>
</li>

</ul>
					</li>
					
				</ul>
			</nav>

			<div id="et_mobile_nav_menu">
				<div class="mobile_nav closed">
					<span class="select_page"></span>
					<span class="mobile_menu_bar"></span>
				</div>
			</div>				
		</div> <!-- #et-top-navigation -->
	</div> <!-- .container -->
	<div class="et_search_outer">
		<div class="container et_search_form_container">
			<form role="search" method="get" class="et-search-form" action="http://cole-maclean.github.io/">
			<input type="search" class="et-search-field" placeholder="搜索 &hellip;" value="" name="s" title="搜索：" /></form>
			<span class="et_close_search_field"></span>
		</div>
	</div>
</header> <!-- #main-header -->

    		<div id="et-main-area">
      			

	
	    
	        
	    
	        
	    
	        
	    
	    
	

	
	    
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	                
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	            
	        
	        
	            
	            
	        
	    
	

<article >
  <header style="background-image: url('/')">

<!--article  >
  <div style="background-image: url('/')"/>
  <header style="background-image: url('')"-->
    <h1 class="title">Recurrent Neural Network Based Subreddit Recommender System</h1>
    <p id="subtitletrans" class="subtitlecontainer">
    <a class="subtitle">2017-01-07</a> | <a class = "subtittle" href=https://www.reddit.com/r/InternetIsBeautiful/comments/5ta27q/this_is_a_neural_network_map_of_the_top_50000/>HN</a>
	 <a class="subtitle"></a> <a class="subtitle">  <i class="fa fa-tags"></i>: <a href="/tags/python/">python</a>, <a href="/tags/tensorflow/">tensorflow</a>, <a href="/tags/rnn/">rnn</a>, <a href="/tags/bokeh/">bokeh</a>, <a href="/tags/eda/">EDA</a>, <a href="/tags/data-munging/">Data Munging</a>, <a href="/tags/deep-learning/">Deep Learning</a>, <a href="/tags/recommender-systems/">Recommender Systems</a></a>
    </p>
  </header>
  <section class="post-content">
  <h2>Introduction</h2>

<p>As part of a project course in my second semester, we were tasked with building a system of our chosing that encorporated or showcased any of the Computational Intelligence techniques we learned about in class. For our project, we decided to investigate the application of Recurrent Neural Networks to the task of building a Subreddit recommender system for Reddit users. In this post, I outline some of the implementation details of the final system. A minimal webapp for the final model can be interacted with <a href="http://ponderinghydrogen.pythonanywhere.com/">here,</a> The final research paper for the project can be found <a href="http://cole-maclean.github.io/blog/files/subreddit-recommender.pdf">here</a> and my collaboraters on the project are Barbara Garza and Suren Oganesian. The github repo for the project can be found <a href="https://github.com/cole-maclean/MAI-CI">here</a> with this jupyter notebook being <a href="https://github.com/cole-maclean/MAI-CI/blob/master/notebooks/blog%20post.ipynb">here.</a></p>

<p><img src="/img/spez.PNG" alt="spez"></p>

<h3>Model Hypothesis</h3>

<p>The goal of the project is to utilize the sequence prediction power of RNN&#39;s to predict possibly interesting subreddits to a user based on their comment history. The hypothesis of the recommender model is, given an ordered sequence of user subreddit interactions, patterns will emerge that favour the discovery of paticular new subreddits given that historical user interaction sequence. Intuitively speaking, as users interact with the Reddit ecosystem, they discover new subreddits of interest, but these new discoveries are influenced by the communities they have previously been interacting with. We can then train a model to recognize these emergent subreddit discoveries based on users historical subreddit discovery patterns. When the model is presented with a new sequence of user interaction, it &quot;remembers&quot; other users that historically had similiar interaction habits and recommends their subreddits that the current user has yet to discover.  </p>

<p>This sequential view of user interaction/subreddit discovery is similiar in structure to other problems being solved with the use of Recurrent Neural Networks, such as <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Character Level Language Modelling</a> and <a href="http://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html">Automatic Authorship Detection</a>. Due to the successes of these similiarly structured problems, we have decided to explore RNN models for the subbreddit Recommendator System.</p>

<h3>The Data</h3>

<p>The secret sauce in any machine learning system, we need data. Reddit provides a convenient API for scrapping its public facing data, and the python package <a href="https://praw.readthedocs.io/en/latest/">PRAW</a> is a popular and well documented wrapper that we used in this project. With the aim of developing sequences of user subreddit interactions, all we need for our raw data is a list of 3-tuples in the form [username,subreddit,utc timestamp]. The following script provides a helper function to collect and store random user comment data from Reddit&#39;s streaming &#39;all&#39; comments. Note that PRAW authentication config data needs to be stored in a file named &#39;secret.ini&#39; with:<br>
[reddit]<br>
api_key: key<br>
client_id: id<br>
client_api_key: client key<br>
redirect_url: redir url<br>
user_agent: subreddit-recommender by /u/upcmaici v 0.0.1  </p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">praw</span>
<span class="kn">import</span> <span class="nn">configparser</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="c">#Import configuration parameters, user agent for PRAW Reddit object</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">configparser</span><span class="o">.</span><span class="n">ConfigParser</span><span class="p">()</span>
<span class="n">config</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="s">'secrets.ini'</span><span class="p">)</span>

<span class="c">#load user agent string</span>
<span class="n">reddit_user_agent</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'reddit'</span><span class="p">,</span> <span class="s">'user_agent'</span><span class="p">)</span>
<span class="n">client_id</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'reddit'</span><span class="p">,</span> <span class="s">'client_id'</span><span class="p">)</span>
<span class="n">client_secret</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'reddit'</span><span class="p">,</span> <span class="s">'client_api_key'</span><span class="p">)</span>

<span class="c">#main data scrapping script</span>
<span class="k">def</span> <span class="nf">scrape_data</span><span class="p">(</span><span class="n">n_scrape_loops</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">reddit_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c">#initialize the praw Reddit object</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">praw</span><span class="o">.</span><span class="n">Reddit</span><span class="p">(</span><span class="n">user_agent</span><span class="o">=</span><span class="n">reddit_user_agent</span><span class="p">,</span><span class="n">client_id</span> <span class="o">=</span> <span class="n">client_id</span><span class="p">,</span><span class="n">client_secret</span><span class="o">=</span><span class="n">client_secret</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">scrape_loop</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_scrape_loops</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">all_comments</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">get_comments</span><span class="p">(</span><span class="s">'all'</span><span class="p">)</span>
            <span class="k">print</span> <span class="p">(</span><span class="s">"Scrape Loop "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">scrape_loop</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">cmt</span> <span class="ow">in</span> <span class="n">all_comments</span><span class="p">:</span>
                <span class="n">user</span> <span class="o">=</span> <span class="n">cmt</span><span class="o">.</span><span class="n">author</span>        
                <span class="k">if</span> <span class="n">user</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">user_comment</span> <span class="ow">in</span> <span class="n">user</span><span class="o">.</span><span class="n">get_comments</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
                        <span class="n">reddit_data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">user</span><span class="o">.</span><span class="n">name</span><span class="p">,</span><span class="n">user_comment</span><span class="o">.</span><span class="n">subreddit</span><span class="o">.</span><span class="n">display_name</span><span class="p">,</span>
                                      <span class="n">user_comment</span><span class="o">.</span><span class="n">created_utc</span><span class="p">])</span>
        <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reddit_data</span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">raw_data</span> <span class="o">=</span> <span class="n">scrape_data</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">Version 3.5.0 of praw is outdated. Version 4.3.0 was released Thursday January 19, 2017.
Scrape Loop 0
Scrape Loop 1
Scrape Loop 2
Scrape Loop 3
Scrape Loop 4
Scrape Loop 5
Scrape Loop 6
Scrape Loop 7
Scrape Loop 8
Scrape Loop 9
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s">"Collected "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">raw_data</span><span class="p">))</span> <span class="o">+</span> <span class="s">" comments"</span><span class="p">)</span>
<span class="n">raw_data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">Collected 158914 comments





[['Illuminate1738', 'MapPorn', 1486680909.0],
 ['Illuminate1738', 'MapPorn', 1486471452.0],
 ['Illuminate1738', 'nova', 1486228887.0],
 ['Illuminate1738', 'nova', 1485554669.0],
 ['Illuminate1738', 'nova', 1485549461.0],
 ['Illuminate1738', 'MapPorn', 1485297397.0],
 ['Illuminate1738', 'ShitRedditSays', 1485261592.0],
 ['Illuminate1738', 'ShittyMapPorn', 1483836164.0],
 ['Illuminate1738', 'MapPorn', 1483798990.0],
 ['Illuminate1738', 'MapPorn', 1483503268.0]]
</code></pre></div>
<h2>Data Munging</h2>

<p>We need to parse the raw data into a structure consumpable by a supervised learning algorithm like RNN&#39;s. First we build a model vocabulary and ditribution of subreddit popularity from the collect raw data. We use this to build the training dataset, the subreddit interaction sequence for each user, ordered and then split into chunks representing different periods of Reddit interaction and discovery. From each chunk, we can randomly remove a single subreddit from the interaction as the &quot;discovered&quot; subreddit and use it as our training label for the interaction sequences. This formulation brings with it a hyperparameter that will require tuning, namely the sequence size of each chunk of user interaction periods. The proposed model utilizes the distribution of subreddits existing in the dataset to weight the random selection of a subreddit as the sequence label, which gives a higher probability of selection to rarer subreddits. This will smoothen the distribution of training labels across the models vocabulary of subreddits in the dataset. Also, each users interaction sequence has been compressed to only represent the sequence of non-repeating subreddits, to eliminate the repeatative structure of users constantly commenting in a single subreddit, while providing information of the users habits in the reddit ecosystem more generally, allowing the model to distinguish broader patterns from the compressed sequences.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">chunks</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">),</span> <span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">lst</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span>
    <span class="n">normed</span> <span class="o">=</span> <span class="p">[</span><span class="n">itm</span><span class="o">/</span><span class="n">s</span> <span class="k">for</span> <span class="n">itm</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">]</span>
    <span class="n">normed</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">normed</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="nb">sum</span><span class="p">(</span><span class="n">normed</span><span class="p">)))</span><span class="c">#pad last value with what ever difference neeeded to make sum to exactly 1</span>
    <span class="k">return</span> <span class="n">normed</span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="s">"""This routine develops the models vocabulary and vocab_probs is also built, representing the inverse probability 
of encounting a paticular subreddit in the given dataset, which is then used to bias the selection of rarer
subreddits as labels to 
smooth the distribution of training labels across all subreddits in the vocabulary"""</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">raw_data</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'user'</span><span class="p">,</span><span class="s">'subreddit'</span><span class="p">,</span><span class="s">'utc_stamp'</span><span class="p">])</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="bp">None</span><span class="c">#free up train_data memory</span>
<span class="n">vocab_counts</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"subreddit"</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">tmp_vocab</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocab_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">total_counts</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">vocab_counts</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">inv_prob</span> <span class="o">=</span> <span class="p">[</span><span class="n">total_counts</span><span class="o">/</span><span class="n">vocab_counts</span><span class="p">[</span><span class="n">sub</span><span class="p">]</span> <span class="k">for</span> <span class="n">sub</span> <span class="ow">in</span> <span class="n">tmp_vocab</span><span class="p">]</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Unseen-Sub"</span><span class="p">]</span> <span class="o">+</span> <span class="n">tmp_vocab</span> <span class="c">#build place holder, Unseen-Sub, for all subs not in vocab</span>
<span class="n">tmp_vocab_probs</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">inv_prob</span><span class="p">)</span>
<span class="c">#force probs sum to 1 by adding differenc to "Unseen-sub" probability</span>
<span class="n">vocab_probs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="o">-</span><span class="nb">sum</span><span class="p">(</span><span class="n">tmp_vocab_probs</span><span class="p">)]</span> <span class="o">+</span> <span class="n">tmp_vocab_probs</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Vocab size = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)))</span>
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">Vocab size = 3546
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">sequence_chunk_size</span> <span class="o">=</span> <span class="mi">15</span>
<span class="k">def</span> <span class="nf">remove_repeating_subs</span><span class="p">(</span><span class="n">raw_data</span><span class="p">):</span>
    <span class="n">cache_data</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">prev_usr</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">past_sub</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">comment_data</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">:</span>
        <span class="n">current_usr</span> <span class="o">=</span> <span class="n">comment_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">current_usr</span> <span class="o">!=</span> <span class="n">prev_usr</span><span class="p">:</span><span class="c">#New user found in sorted comment data, begin sequence extraction for new user</span>
            <span class="k">if</span> <span class="n">prev_usr</span> <span class="o">!=</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">prev_usr</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">cache_data</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span><span class="c">#dump sequences to cache for previous user if not in cache</span>
                <span class="n">cache_data</span><span class="p">[</span><span class="n">prev_usr</span><span class="p">]</span> <span class="o">=</span> <span class="n">usr_sub_seq</span>
            <span class="n">usr_sub_seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">comment_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="c">#initialize user sub sequence list with first sub for current user</span>
            <span class="n">past_sub</span> <span class="o">=</span> <span class="n">comment_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span><span class="c">#if still iterating through the same user, add new sub to sequence if not a repeat</span>
            <span class="k">if</span> <span class="n">comment_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">past_sub</span><span class="p">:</span><span class="c">#Check that next sub comment is not a repeat of the last interacted with sub,</span>
                                            <span class="c">#filtering out repeated interactions</span>
                <span class="n">usr_sub_seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">comment_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">past_sub</span> <span class="o">=</span> <span class="n">comment_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">prev_usr</span> <span class="o">=</span> <span class="n">current_usr</span> <span class="c">#update previous user to being the current one before looping to next comment</span>
    <span class="k">return</span> <span class="n">cache_data</span>

<span class="k">def</span> <span class="nf">build_training_sequences</span><span class="p">(</span><span class="n">usr_data</span><span class="p">):</span>
    <span class="n">train_seqs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c">#split user sub sequences into provided chunks of size sequence_chunk_size</span>
    <span class="k">for</span> <span class="n">usr</span><span class="p">,</span><span class="n">usr_sub_seq</span> <span class="ow">in</span> <span class="n">usr_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">comment_chunks</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">(</span><span class="n">usr_sub_seq</span><span class="p">,</span><span class="n">sequence_chunk_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">chnk</span> <span class="ow">in</span> <span class="n">comment_chunks</span><span class="p">:</span>
            <span class="c">#for each chunk, filter out potential labels to select as training label, filter by the top subs filter list</span>
            <span class="n">filtered_subs</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub</span> <span class="ow">in</span> <span class="n">chnk</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">filtered_subs</span><span class="p">:</span>
                <span class="c">#randomly select the label from filtered subs, using the vocab probability distribution to smooth out</span>
                <span class="c">#representation of subreddit labels</span>
                <span class="n">filter_probs</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">([</span><span class="n">vocab_probs</span><span class="p">[</span><span class="n">sub_indx</span><span class="p">]</span> <span class="k">for</span> <span class="n">sub_indx</span> <span class="ow">in</span> <span class="n">filtered_subs</span><span class="p">])</span>
                <span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">filtered_subs</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">p</span><span class="o">=</span><span class="n">filter_probs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="c">#build sequence by ensuring users sub exists in models vocabulary and filtering out the selected</span>
                <span class="c">#label for this subreddit sequence</span>
                <span class="n">chnk_seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub</span> <span class="ow">in</span> <span class="n">chnk</span> <span class="k">if</span> <span class="n">sub</span> <span class="ow">in</span> <span class="n">vocab</span> <span class="ow">and</span> <span class="n">vocab</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span> <span class="o">!=</span> <span class="n">label</span><span class="p">]</span> 
                <span class="n">train_seqs</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">chnk_seq</span><span class="p">,</span><span class="n">label</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">chnk_seq</span><span class="p">)])</span> 
    <span class="k">return</span> <span class="n">train_seqs</span>
</code></pre></div>
<p>We transform the munged-data into a pandas dataframe for easier manipulation. Note that the subreddits have been integer encoded, indexed by their order in the vocabulary.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">pp_user_data</span> <span class="o">=</span> <span class="n">remove_repeating_subs</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">build_training_sequences</span><span class="p">(</span><span class="n">pp_user_data</span><span class="p">)</span>
<span class="n">seqs</span><span class="p">,</span><span class="n">lbls</span><span class="p">,</span><span class="n">lngths</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'sub_seqs'</span><span class="p">:</span><span class="n">seqs</span><span class="p">,</span>
                         <span class="s">'sub_label'</span><span class="p">:</span><span class="n">lbls</span><span class="p">,</span>
                         <span class="s">'seq_length'</span><span class="p">:</span><span class="n">lngths</span><span class="p">})</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>seq_length</th>
      <th>sub_label</th>
      <th>sub_seqs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>13</td>
      <td>432</td>
      <td>[46, 157, 46, 483, 157, 46, 157, 856, 157, 856...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9</td>
      <td>46</td>
      <td>[157, 432, 157, 432, 157, 432, 157, 157, 157]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7</td>
      <td>46</td>
      <td>[432, 432, 432, 432, 856, 856, 157]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13</td>
      <td>432</td>
      <td>[46, 157, 46, 157, 46, 157, 856, 157, 46, 157,...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13</td>
      <td>1048</td>
      <td>[46, 157, 46, 157, 46, 157, 46, 157, 46, 157, ...</td>
    </tr>
  </tbody>
</table>
</div>

<h3>Tensorflow Model Architecture</h3>

<p>Originally, we built the model directly on-top of tensorflow, using the fantastic tutorials from <a href="http://r2rt.com/">R2RT</a> as reference. However, building and managing various neural network architectures with Tensorflow can be cumbersome, and higher level wrapper packages exist to abstract away some of the more tedious variable and graph definition steps required for tensorflow models. We chose the <a href="http://tflearn.org/">tflearn</a> python package, which has an API similiar to sklearn, which the team had more experience with. With tflearn, it&#39;s rather easy to plug and play with different layers, and we experimented with LSTM, GRU and multi-layered Bi-Directional RNN architectures.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tflearn</span>
<span class="kn">from</span> <span class="nn">tflearn.data_utils</span> <span class="kn">import</span> <span class="n">to_categorical</span><span class="p">,</span> <span class="n">pad_sequences</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">test</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">n_epoch</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">n_units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">):</span>

    <span class="n">trainX</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'sub_seqs'</span><span class="p">]</span>
    <span class="n">trainY</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'sub_label'</span><span class="p">]</span>
    <span class="n">testX</span> <span class="o">=</span>  <span class="n">test</span><span class="p">[</span><span class="s">'sub_seqs'</span><span class="p">]</span>
    <span class="n">testY</span> <span class="o">=</span>  <span class="n">test</span><span class="p">[</span><span class="s">'sub_label'</span><span class="p">]</span>

    <span class="c"># Sequence padding</span>
    <span class="n">trainX</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">sequence_chunk_size</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="s">'post'</span><span class="p">)</span>
    <span class="n">testX</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">sequence_chunk_size</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="s">'post'</span><span class="p">)</span>

    <span class="c"># Converting labels to binary vectors</span>
    <span class="n">trainY</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">trainY</span><span class="p">,</span> <span class="n">nb_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
    <span class="n">testY</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">testY</span><span class="p">,</span> <span class="n">nb_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>

    <span class="c"># Network building</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">input_data</span><span class="p">([</span><span class="bp">None</span><span class="p">,</span> <span class="mi">15</span><span class="p">])</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span><span class="n">weights_init</span><span class="o">=</span><span class="n">tflearn</span><span class="o">.</span><span class="n">initializations</span><span class="o">.</span><span class="n">xavier</span><span class="p">(),</span><span class="n">return_seq</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">,</span><span class="n">weights_init</span><span class="o">=</span><span class="n">tflearn</span><span class="o">.</span><span class="n">initializations</span><span class="o">.</span><span class="n">xavier</span><span class="p">())</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">regression</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                             <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">)</span>

    <span class="c"># Training</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">DNN</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">tensorboard_verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">,</span> <span class="n">validation_set</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testY</span><span class="p">),</span> <span class="n">show_metric</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span><span class="n">n_epoch</span><span class="o">=</span><span class="n">n_epoch</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">c:\python35\lib\site-packages\tensorflow\python\util\deprecation.py:155: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead
  arg_spec = inspect.getargspec(func)
c:\python35\lib\site-packages\tensorflow\python\util\deprecation.py:155: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead
  arg_spec = inspect.getargspec(func)
c:\python35\lib\site-packages\tensorflow\contrib\labeled_tensor\python\ops\_typecheck.py:233: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead
  spec = inspect.getargspec(f)
</code></pre></div>
<h2>Model Training</h2>

<p>We split the model into train/test sets and begin training. Here we use the default training parameters, but the model can be tuned for epochs, internal units, dropout, learning-rate and other hyperparameters of the chosen RNN structure.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">split_perc</span><span class="o">=</span><span class="mf">0.8</span>
<span class="n">train_len</span><span class="p">,</span> <span class="n">test_len</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span><span class="o">*</span><span class="n">split_perc</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">split_perc</span><span class="p">))</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">ix</span><span class="p">[:</span><span class="n">train_len</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">train_df</span><span class="o">.</span><span class="n">ix</span><span class="p">[</span><span class="n">train_len</span><span class="p">:</span><span class="n">train_len</span> <span class="o">+</span> <span class="n">test_len</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">test</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">Training Step: 29  | total loss: [1m[32m8.17396[0m[0m | time: 1.104s
| Adam | epoch: 002 | loss: 8.17396 -- iter: 3584/3775
Training Step: 30  | total loss: [1m[32m8.17391[0m[0m | time: 2.222s
| Adam | epoch: 002 | loss: 8.17391 | val_loss: 8.17437 -- iter: 3775/3775
--
</code></pre></div>
<p>It can be difficult to tell how well the model is performing simply by staring at the flipping numbers above, but tensorflow provides a visualization tool called <a href="https://www.tensorflow.org/how_tos/summaries_and_tensorboard/">tensorboard</a> and tflearn has different prebuilt dashboards which can be changed using the tensorboard_verbose option of the DNN layer.  </p>

<p><img src="/img/tensorboard.PNG" alt="tensorboard"></p>

<h2>Visualizng the Model</h2>

<p>As part of the model, a high dimension embedding space is learnt representing the subreddits in the vocabulary as vectors that can be reasoned about with &quot;distance&quot; from each other in the embedding space, and visualized with dimensionality reduction techniques, similiar to the concepts used in <a href="http://www.deeplearningweekly.com/blog/demystifying-word2vec">word2vec.</a> The tutorial by Arthur Juliani <a href="https://medium.com/@awjuliani/visualizing-deep-learning-with-t-sne-tutorial-and-video-e7c59ee4080c#.xdlzpd34w">here</a> was used to build the embedding visualization.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="c">#retrieve the embedding layer fro mthe model by default name 'Embedding'</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">get_layer_variables_by_name</span><span class="p">(</span><span class="s">"Embedding"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">finalWs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">lowDWeights</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">finalWs</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">bokeh.plotting</span> <span class="kn">import</span> <span class="n">figure</span><span class="p">,</span> <span class="n">show</span><span class="p">,</span> <span class="n">output_notebook</span><span class="p">,</span><span class="n">output_file</span>
<span class="kn">from</span> <span class="nn">bokeh.models</span> <span class="kn">import</span> <span class="n">ColumnDataSource</span><span class="p">,</span> <span class="n">LabelSet</span>

<span class="c">#control the number of labelled subreddits to display</span>
<span class="n">sparse_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">lbl</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span><span class="mf">0.01</span> <span class="k">else</span> <span class="s">''</span> <span class="k">for</span> <span class="n">lbl</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">]</span>
<span class="n">source</span> <span class="o">=</span> <span class="n">ColumnDataSource</span><span class="p">({</span><span class="s">'x'</span><span class="p">:</span><span class="n">lowDWeights</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="s">'y'</span><span class="p">:</span><span class="n">lowDWeights</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s">'labels'</span><span class="p">:</span><span class="n">sparse_labels</span><span class="p">})</span>


<span class="n">TOOLS</span><span class="o">=</span><span class="s">"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,tap,save,box_select,poly_select,lasso_select,"</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">figure</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">TOOLS</span><span class="p">)</span>

<span class="n">p</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">"x"</span><span class="p">,</span> <span class="s">"y"</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fill_alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
          <span class="n">line_color</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">LabelSet</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"y"</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s">"labels"</span><span class="p">,</span> <span class="n">y_offset</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                  <span class="n">text_font_size</span><span class="o">=</span><span class="s">"10pt"</span><span class="p">,</span> <span class="n">text_color</span><span class="o">=</span><span class="s">"#555555"</span><span class="p">,</span> <span class="n">text_align</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span>
                 <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>
<span class="n">p</span><span class="o">.</span><span class="n">add_layout</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c">#output_file("embedding.html")</span>
<span class="n">output_notebook</span><span class="p">()</span>
<span class="n">show</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</code></pre></div>
<p><img src="/img/embedding.PNG" alt="embedding"></p>

<h2>Saving the Model</h2>

<p>To save the model for use in making real-world predictions, potentially as part of a webapp, we need to freeze the tensorflow graph and transform the variables into constants to maintain the final network. The tutorial <a href="https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc#.eopkd8pys">here</a> walks us through how to accomplish this.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">graph_util</span>
<span class="k">def</span> <span class="nf">freeze_graph</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c"># We precise the file fullname of our freezed graph</span>
    <span class="n">output_graph</span> <span class="o">=</span> <span class="s">"/tmp/frozen_model.pb"</span>

    <span class="c"># Before exporting our graph, we need to precise what is our output node</span>
    <span class="c"># This is how TF decides what part of the Graph he has to keep and what part it can dump</span>
    <span class="c"># NOTE: this variable is plural, because you can have multiple output nodes</span>
    <span class="n">output_node_names</span> <span class="o">=</span> <span class="s">"InputData/X,FullyConnected/Softmax"</span>

    <span class="c"># We clear devices to allow TensorFlow to control on which device it will load operations</span>
    <span class="n">clear_devices</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="c"># We import the meta graph and retrieve a Saver</span>
    <span class="c">#saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)</span>

    <span class="c"># We retrieve the protobuf graph definition</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">graph</span>
    <span class="n">input_graph_def</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">()</span>

    <span class="c"># We start a session and restore the graph weights</span>
    <span class="c"># We use a built-in TF helper to export variables to constants</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">session</span>
    <span class="n">output_graph_def</span> <span class="o">=</span> <span class="n">graph_util</span><span class="o">.</span><span class="n">convert_variables_to_constants</span><span class="p">(</span>
        <span class="n">sess</span><span class="p">,</span> <span class="c"># The session is used to retrieve the weights</span>
        <span class="n">input_graph_def</span><span class="p">,</span> <span class="c"># The graph_def is used to retrieve the nodes </span>
        <span class="n">output_node_names</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">","</span><span class="p">)</span> <span class="c"># The output node names are used to select the usefull nodes</span>
    <span class="p">)</span> 

    <span class="c"># Finally we serialize and dump the output graph to the filesystem</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">output_graph</span><span class="p">,</span> <span class="s">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">output_graph_def</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="si">%</span><span class="s">d ops in the final graph."</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_graph_def</span><span class="o">.</span><span class="n">node</span><span class="p">))</span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">freeze_graph</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">INFO:tensorflow:Froze 152 variables.
Converted 8 variables to const ops.
607 ops in the final graph.
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">load_graph</span><span class="p">(</span><span class="n">frozen_graph_filename</span><span class="p">):</span>
    <span class="c"># We load the protobuf file from the disk and parse it to retrieve the </span>
    <span class="c"># unserialized graph_def</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">frozen_graph_filename</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">graph_def</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">GraphDef</span><span class="p">()</span>
        <span class="n">graph_def</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>

    <span class="c"># Then, we can use again a convenient built-in function to import a graph_def into the </span>
    <span class="c"># current default Graph</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">()</span> <span class="k">as</span> <span class="n">graph</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">import_graph_def</span><span class="p">(</span>
            <span class="n">graph_def</span><span class="p">,</span> 
            <span class="n">input_map</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
            <span class="n">return_elements</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
            <span class="n">name</span><span class="o">=</span><span class="s">"prefix"</span><span class="p">,</span> 
            <span class="n">op_dict</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
            <span class="n">producer_op_list</span><span class="o">=</span><span class="bp">None</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">graph</span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">grph</span> <span class="o">=</span> <span class="n">load_graph</span><span class="p">(</span><span class="s">"/tmp/frozen_model.pb"</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">x</span> <span class="o">=</span> <span class="n">grph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s">'prefix/InputData/X:0'</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">grph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s">"prefix/FullyConnected/Softmax:0"</span><span class="p">)</span>

<span class="c"># We launch a Session</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">grph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c"># Note: we didn't initialize/restore anything, everything is stored in the graph_def</span>
    <span class="n">y_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">sequence_chunk_size</span><span class="p">]</span> 
    <span class="p">})</span>
    <span class="k">print</span><span class="p">(</span><span class="n">y_out</span><span class="p">)</span> <span class="c"># [[ False ]] Yay, it works!</span>
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">[[ 0.00028193  0.00028239  0.00028253 ...,  0.00028406  0.00028214
   0.00028208]]
</code></pre></div>
<h2>Final Recommender</h2>

<p>Using the frozen model, we can predict the most likely subreddits to be of interest to a user by collecting Reddit data for a specific user and provide final recommendations based on the most common subreddits with the highest probabilities from the RNN predictions for each of the subreddit sequence chunks of the user.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="k">def</span> <span class="nf">collect_user_data</span><span class="p">(</span><span class="n">user</span><span class="p">):</span>
    <span class="c">#Import configuration parameters, user agent for PRAW Reddit object</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">configparser</span><span class="o">.</span><span class="n">ConfigParser</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="s">'secrets.ini'</span><span class="p">)</span>

    <span class="c">#load user agent string</span>
    <span class="n">reddit_user_agent</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'reddit'</span><span class="p">,</span> <span class="s">'user_agent'</span><span class="p">)</span>
    <span class="n">client_id</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'reddit'</span><span class="p">,</span> <span class="s">'client_id'</span><span class="p">)</span>
    <span class="n">client_secret</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'reddit'</span><span class="p">,</span> <span class="s">'client_api_key'</span><span class="p">)</span>
    <span class="c">#initialize the praw Reddit object</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">praw</span><span class="o">.</span><span class="n">Reddit</span><span class="p">(</span><span class="n">user_agent</span><span class="o">=</span><span class="n">reddit_user_agent</span><span class="p">,</span><span class="n">client_id</span> <span class="o">=</span> <span class="n">client_id</span><span class="p">,</span><span class="n">client_secret</span><span class="o">=</span><span class="n">client_secret</span><span class="p">)</span> 
    <span class="n">praw_user</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">get_redditor</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>
    <span class="n">user_data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">user_comment</span><span class="o">.</span><span class="n">subreddit</span><span class="o">.</span><span class="n">display_name</span><span class="p">,</span>
                  <span class="n">user_comment</span><span class="o">.</span><span class="n">created_utc</span><span class="p">)</span> <span class="k">for</span> <span class="n">user_comment</span> <span class="ow">in</span> <span class="n">praw_user</span><span class="o">.</span><span class="n">get_comments</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="bp">None</span><span class="p">)]</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">user_data</span><span class="p">,</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c">#sort by ascending utc timestamp</span>

<span class="k">def</span> <span class="nf">user_recs</span><span class="p">(</span><span class="n">user</span><span class="p">,</span><span class="n">n_recs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">chunk_size</span><span class="o">=</span><span class="n">sequence_chunk_size</span><span class="p">):</span>
    <span class="n">user_data</span> <span class="o">=</span> <span class="n">collect_user_data</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>
    <span class="n">user_sub_seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">vocab</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">user_data</span><span class="p">]</span>
    <span class="n">non_repeating_subs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">sub</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">user_sub_seq</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">non_repeating_subs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">sub</span> <span class="o">!=</span> <span class="n">user_sub_seq</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">non_repeating_subs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span>
    <span class="n">user_subs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">vocab</span><span class="p">[</span><span class="n">sub_index</span><span class="p">]</span> <span class="k">for</span> <span class="n">sub_index</span> <span class="ow">in</span> <span class="n">non_repeating_subs</span><span class="p">])</span>
    <span class="n">sub_chunks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">chunks</span><span class="p">(</span><span class="n">non_repeating_subs</span><span class="p">,</span><span class="n">chunk_size</span><span class="p">))</span>
    <span class="n">user_input</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sub_chunks</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="s">'post'</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">grph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s">'prefix/InputData/X:0'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">grph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s">"prefix/FullyConnected/Softmax:0"</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">grph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">sub_probs</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">user_input</span>
        <span class="p">})</span>
    <span class="c">#select the subreddit with highest prediction prob for each of the input subreddit sequences of the user</span>
    <span class="n">recs</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span> <span class="k">for</span> <span class="n">probs</span> <span class="ow">in</span> <span class="n">sub_probs</span><span class="p">]</span>
    <span class="n">filtered_recs</span> <span class="o">=</span> <span class="p">[</span><span class="n">filt_rec</span> <span class="k">for</span> <span class="n">filt_rec</span> <span class="ow">in</span> <span class="n">recs</span> <span class="k">if</span> <span class="n">filt_rec</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">user_sub_seq</span><span class="p">]</span>
    <span class="n">top_x_recs</span><span class="p">,</span><span class="n">cnt</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">Counter</span><span class="p">(</span><span class="n">filtered_recs</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">n_recs</span><span class="p">))</span>
    <span class="n">sub_recs</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">sub_index</span><span class="p">]</span> <span class="k">for</span> <span class="n">sub_index</span> <span class="ow">in</span> <span class="n">top_x_recs</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">sub_recs</span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">user_recs</span><span class="p">(</span><span class="s">"ponderinghydrogen"</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">['fantasyfootball', 'PS3']
</code></pre></div>
<h2>The Web App</h2>

<p>Those are all the pieces required to build a functioning subreddit recommender system that users can try! Using Flask, a simple web app can be made taking as input any valid reddit user name and outputting recommendations for that user. A minimal web app doing just that can be interacted with <a href="http://ponderinghydrogen.pythonanywhere.com/">here</a></p>

<p><img src="/img/wepapp.PNG" alt="webapp"></p>

<h2>Final Thoughts</h2>

<p>The model being served in the above webapp is an under-tuned and under-dataed proof-of-concept single layer RNN, but it is still  surprisingly capable of suggesting interesting subreddits to some testers I&#39;ve had use the app. Neural Networks really are powerful methods for tackling difficult problems, and with better and better Machine Learning research and tooling being released daily, and increasingly powerful computers, the pool of potential problems solvable by a group of determined engineers keeps getting larger. I&#39;m looking forward to tackling the next one.</p>

  </section>
  
</article>

<!-- Post navigation -->


<!-- Disqus -->


    		</div> <!-- #et-main-area -->
		</div> <!-- #page-container -->
		
<script src="/js/katex_init.js"></script>



<footer id="page-footer-trans" class="page-footer">
	<!--<p class="text"><a href="http://cole-maclean.github.io/">Cole MacLean</a>
</p>-->
	<p class="text">Hosted by <a href="https://github.com">GitHub</a>. Theme created by <a href="https://github.com/fengzhichu/fengzhichu-theme/tree/gh-pages">枫之楚</a></p>
</footer>


    
<!-- WP Audio player plugin v1.9.3 - https://www.tipsandtricks-hq.com/wordpress-audio-music-player-plugin-4556/ -->
    <script type="text/javascript">
        soundManager.useFlashBlock = true; // optional - if used, required flashblock.css
        soundManager.url = '/plugins/soundmanager2.swf';
        function play_mp3(flg, ids, mp3url, volume, loops)
        {
            //Check the file URL parameter value
            var pieces = mp3url.split("|");
            if (pieces.length > 1) {//We have got an .ogg file too
                mp3file = pieces[0];
                oggfile = pieces[1];
                //set the file URL to be an array with the mp3 and ogg file
                mp3url = new Array(mp3file, oggfile);
            }

            soundManager.createSound({
                id: 'btnplay_' + ids,
                volume: volume,
                url: mp3url
            });

            if (flg == 'play') {
                    soundManager.play('btnplay_' + ids, {
                    onfinish: function() {
                        if (loops == 'true') {
                            loopSound('btnplay_' + ids);
                        }
                        else {
                            document.getElementById('btnplay_' + ids).style.display = 'inline';
                            document.getElementById('btnstop_' + ids).style.display = 'none';
                        }
                    }
                });
            }
            else if (flg == 'stop') {
    //soundManager.stop('btnplay_'+ids);
                soundManager.pause('btnplay_' + ids);
            }
        }
        function show_hide(flag, ids)
        {
            if (flag == 'play') {
                document.getElementById('btnplay_' + ids).style.display = 'none';
                document.getElementById('btnstop_' + ids).style.display = 'inline';
            }
            else if (flag == 'stop') {
                document.getElementById('btnplay_' + ids).style.display = 'inline';
                document.getElementById('btnstop_' + ids).style.display = 'none';
            }
        }
        function loopSound(soundID)
        {
            window.setTimeout(function() {
                soundManager.play(soundID, {onfinish: function() {
                        loopSound(soundID);
                    }});
            }, 1);
        }
        function stop_all_tracks()
        {
            soundManager.stopAll();
            var inputs = document.getElementsByTagName("input");
            for (var i = 0; i < inputs.length; i++) {
                if (inputs[i].id.indexOf("btnplay_") == 0) {
                    inputs[i].style.display = 'inline';//Toggle the play button
                }
                if (inputs[i].id.indexOf("btnstop_") == 0) {
                    inputs[i].style.display = 'none';//Hide the stop button
                }
            }
        }
    </script>
    <script type='text/javascript' src="/js/frontend-builder-global-functions.js"></script>
<script type='text/javascript' src="/js/custom.js"></script>
<script type='text/javascript' src="/js/jquery.fitvids.js"></script>
<script type='text/javascript' src="/js/waypoints.min.js"></script>
<script type='text/javascript' src="/js/jquery.magnific-popup.js"></script>
<script type='text/javascript'>
/* <![CDATA[ */
var et_custom = {"ajaxurl":"http:\/\/blog.ibireme.com\/wp-admin\/admin-ajax.php","images_uri":"http:\/\/blog.ibireme.com\/wp-content\/themes\/Divi\/images","builder_images_uri":"http:\/\/blog.ibireme.com\/wp-content\/themes\/Divi\/includes\/builder\/images","et_load_nonce":"ae39988568","subscription_failed":"Please, check the fields below to make sure you entered the correct information.","fill":"Fill","field":"field","invalid":"Invalid email","captcha":"Captcha","prev":"Prev","previous":"\u4e0a\u4e00\u9875","next":"\u4e0b\u4e00\u9875","is_builder_plugin_used":""};
/* ]]> */
</script>
<script type='text/javascript' src="/js/frontend-builder-scripts.js"></script>
  </body>
</html>
